{"version":"1","records":[{"hierarchy":{"lvl1":"Scikit-learn on Argo Observations"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Scikit-learn on Argo Observations"},"content":"\n\n\n\n\n\n\n\n\n\nThis Project Pythia Cookbook covers two objectives:\n\nAccessing publicly available, quality-controlled \n\nBiogeochemical-Argo ocean observations\n\nDemonstrating uses of \n\nscikit-learn, a powerful Python package for machine learning.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Scikit-learn on Argo Observations","lvl2":"Motivation"},"type":"lvl2","url":"/#motivation","position":2},{"hierarchy":{"lvl1":"Scikit-learn on Argo Observations","lvl2":"Motivation"},"content":"This cookbook provides an overview of how to use python to access Argo oceanographic data and how to use sklearn to perform machine learning analyses. Argo is a global observatory of in situ robots that autonomously sample the ocean interior. It is an international collaborative effort, and provides a treasure trove of high quality, open-source data. However, there are many different ways to access Argo data, which can get confusing for users. This cookbook highlights some basic workflows to access and work with Argo data.","type":"content","url":"/#motivation","position":3},{"hierarchy":{"lvl1":"Scikit-learn on Argo Observations","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":4},{"hierarchy":{"lvl1":"Scikit-learn on Argo Observations","lvl2":"Authors"},"content":"Song Sangmin, \n\nMichael Chen.","type":"content","url":"/#authors","position":5},{"hierarchy":{"lvl1":"Scikit-learn on Argo Observations","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":6},{"hierarchy":{"lvl1":"Scikit-learn on Argo Observations","lvl3":"Contributors","lvl2":"Authors"},"content":"","type":"content","url":"/#contributors","position":7},{"hierarchy":{"lvl1":"Scikit-learn on Argo Observations","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":8},{"hierarchy":{"lvl1":"Scikit-learn on Argo Observations","lvl2":"Structure"},"content":"This cookbook is broken up into two main sections.\n\nArgo Foundations\n\nScikit-learn Workflows","type":"content","url":"/#structure","position":9},{"hierarchy":{"lvl1":"Scikit-learn on Argo Observations","lvl3":"Section 1: Argo Foundations","lvl2":"Structure"},"type":"lvl3","url":"/#section-1-argo-foundations","position":10},{"hierarchy":{"lvl1":"Scikit-learn on Argo Observations","lvl3":"Section 1: Argo Foundations","lvl2":"Structure"},"content":"This section contains two notebooks. argo-introductions.ipynb provides an overview of the Argo program, what kind of data are available, and how the data are structured. The argo-access.ipynb provides an overview of several methods to retrieve Argo data.","type":"content","url":"/#section-1-argo-foundations","position":11},{"hierarchy":{"lvl1":"Scikit-learn on Argo Observations","lvl3":"Section 2: Scikit-learn Workflows","lvl2":"Structure"},"type":"lvl3","url":"/#section-2-scikit-learn-workflows","position":12},{"hierarchy":{"lvl1":"Scikit-learn on Argo Observations","lvl3":"Section 2: Scikit-learn Workflows","lvl2":"Structure"},"content":"This section provides an overview of workflows using the sklearn package to conduct machine learning analyses on Argo data. The notebooks provide workflows on running regression and clustering (under construction) analyses.","type":"content","url":"/#section-2-scikit-learn-workflows","position":13},{"hierarchy":{"lvl1":"Scikit-learn on Argo Observations","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":14},{"hierarchy":{"lvl1":"Scikit-learn on Argo Observations","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using \n\nBinder or on your local machine.","type":"content","url":"/#running-the-notebooks","position":15},{"hierarchy":{"lvl1":"Scikit-learn on Argo Observations","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":16},{"hierarchy":{"lvl1":"Scikit-learn on Argo Observations","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nBinder, which enables the execution of a\n\n\nJupyter Book in the cloud. The details of how this works are not\nimportant for now. All you need to know is how to launch a Pythia\nCookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\n“launch Binder”. After a moment you should be presented with a\nnotebook that you can interact with. I.e. you’ll be able to execute\nand even change the example programs. You’ll see that the code cells\nhave no output at first, until you execute them by pressing\nShift+Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.","type":"content","url":"/#running-on-binder","position":17},{"hierarchy":{"lvl1":"Scikit-learn on Argo Observations","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":18},{"hierarchy":{"lvl1":"Scikit-learn on Argo Observations","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer, you will need to follow this workflow:\n\n(Replace “cookbook-example” with the title of your cookbooks)\n\nClone the https://github.com/ProjectPythia/cookbook-example repository: git clone https://github.com/ProjectPythia/cookbook-example.git\n\nMove into the cookbook-example directorycd cookbook-example\n\nCreate and activate your conda environment from the environment.yml fileconda env create -f environment.yml\nconda activate cookbook-example\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab","type":"content","url":"/#running-on-your-own-machine","position":19},{"hierarchy":{"lvl1":"Accessing Argo Data"},"type":"lvl1","url":"/notebooks/argo-access","position":0},{"hierarchy":{"lvl1":"Accessing Argo Data"},"content":"\n\n","type":"content","url":"/notebooks/argo-access","position":1},{"hierarchy":{"lvl1":"Accessing Argo Data"},"type":"lvl1","url":"/notebooks/argo-access#accessing-argo-data","position":2},{"hierarchy":{"lvl1":"Accessing Argo Data"},"content":"\n\n\n\n","type":"content","url":"/notebooks/argo-access#accessing-argo-data","position":3},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/argo-access#overview","position":4},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl2":"Overview"},"content":"Building upon previous notebook, \n\nIntroduction to Argo, we next explore how to access Argo data using various methods.\n\nThese methods are described in more detail on their respective websites, linked below. Our goal here is to provide a brief overview of some of the different tools available.\n\nGO-BGC Toolbox\n\nArgopy, a dedicated Python package\n\nArgovis for API-based queries 2. Downloading [monthly snapshots](http://www.argodatamgt.org/Access-to-data/Argo-DOI-Digital-Object-Identifier) using Argo DOI's  4. Using the [GO-BGC Toolbox](https://github.com/go-bgc/workshop-python) \n\nAfter going through this notebook, you will be able to retrieve Argo data of interest within a certain time frame, geographical location, or by platform identifier. There are many other ways of working with Argo data, so we encourage users to explore what applications work best for their needs.\nFurther information on Argo access can be found on the \n\nArgo website.\n\n","type":"content","url":"/notebooks/argo-access#overview","position":5},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/argo-access#prerequisites","position":6},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl2":"Prerequisites"},"content":"Label the importance of each concept explicitly as helpful/necessary.\n\nConcepts\n\nImportance\n\nNotes\n\nIntro to Numpy\n\nNecessary\n\n\n\nIntro to NetCDF\n\nNecessary\n\nFamiliarity with metadata structure\n\nIntro to Xarray\n\nNecessary\n\n\n\nTime to learn: 20 min\n\n\n\n","type":"content","url":"/notebooks/argo-access#prerequisites","position":7},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/argo-access#imports","position":8},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl2":"Imports"},"content":"Begin your body of content with another --- divider before continuing into this section, then remove this body text and populate the following code cell with all necessary Python imports up-front:\n\n# Import packages\nimport sys\nimport os\nimport numpy as np\nimport pandas as pd\nimport scipy\nimport xarray as xr\nfrom datetime import datetime, timedelta\n\nimport requests\nimport time\nimport urllib3\nimport shutil\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport seaborn as sns\nfrom cmocean import cm as cmo\n\nfrom argovisHelpers import helpers as avh\n\n","type":"content","url":"/notebooks/argo-access#imports","position":9},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl2":"1. Downloading with the GO-BGC Toolbox"},"type":"lvl2","url":"/notebooks/argo-access#id-1-downloading-with-the-go-bgc-toolbox","position":10},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl2":"1. Downloading with the GO-BGC Toolbox"},"content":"In the previous notebook, \n\nIntroduction to Argo, we saw how Argo synthetic profile (‘\n\nsprof’) data is stored in netcdf4 format.\n\nUsing the GDAC function allows you to subset and download Sprof’s for multiple floats.\nWe recommend this tool for users who only need a few profilesd in a specific area of interest.\nConsiderations:\n\nEasy to use and understand\n\nDownloads float data as individual .nc files to your local machine (takes up storage space)\n\nMust download all variables available (cannot subset only variables of interest)\n\nThe two major functions below are courtesy of the \n\nGO-BGC Toolbox (Ethan Campbell). A full tutorial is available in the Toolbox.\n\n# # Base filepath. Need for Argo GDAC function.z\n# root = '/Users/sangminsong/Library/CloudStorage/OneDrive-UW/Code/2024_Pythia/'\n# profile_dir = root + 'SOCCOM_GO-BGC_LoResQC_LIAR_28Aug2023_netcdf/'\n\n# # Base filepath. Need for Argo GDAC function.\nroot = '../data/'\nprofile_dir = root + 'bgc-argo/'\n\n\n\n","type":"content","url":"/notebooks/argo-access#id-1-downloading-with-the-go-bgc-toolbox","position":11},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"1.0 GO-BGC Toolbox Functions","lvl2":"1. Downloading with the GO-BGC Toolbox"},"type":"lvl3","url":"/notebooks/argo-access#id-1-0-go-bgc-toolbox-functions","position":12},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"1.0 GO-BGC Toolbox Functions","lvl2":"1. Downloading with the GO-BGC Toolbox"},"content":"\n\n# Function to download a single file (From GO-BGC Toolbox)\ndef download_file(url_path,filename,save_to=None,overwrite=False,verbose=True):\n    \"\"\" Downloads and saves a file from a given URL using HTTP protocol.\n\n    Note: If '404 file not found' error returned, function will return without downloading anything.\n    \n    Arguments:\n        url_path: root URL to download from including trailing slash ('/')\n        filename: filename to download including suffix\n        save_to: None (to download to root Google Drive GO-BGC directory)\n                 or directory path\n        overwrite: False to leave existing files in place\n                   or True to overwrite existing files\n        verbose: True to announce progress\n                 or False to stay silent\n    \n    \"\"\"\n    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\n    if save_to is None:\n      save_to = root #profile_dir  # EDITED HERE\n\n    try:\n      if filename in os.listdir(save_to):\n          if not overwrite:\n              if verbose: print('>>> File ' + filename + ' already exists. Leaving current version.')\n              return\n          else:\n              if verbose: print('>>> File ' + filename + ' already exists. Overwriting with new version.')\n\n      def get_func(url,stream=True):\n          try:\n              return requests.get(url,stream=stream,auth=None,verify=False)\n          except requests.exceptions.ConnectionError as error_tag:\n              print('Error connecting:',error_tag)\n              time.sleep(1)\n              return get_func(url,stream=stream)\n\n      response = get_func(url_path + filename,stream=True)\n\n      if response.status_code == 404:\n          if verbose: print('>>> File ' + filename + ' returned 404 error during download.')\n          return\n      with open(save_to + filename,'wb') as out_file:\n          shutil.copyfileobj(response.raw,out_file)\n      del response\n      if verbose: print('>>> Successfully downloaded ' + filename + '.')\n\n    except:\n      if verbose: print('>>> An error occurred while trying to download ' + filename + '.')\n\n# Function to download and parse GDAC synthetic profile index file (GO-BGC Toolbox)\ndef argo_gdac(lat_range=None,lon_range=None,start_date=None,end_date=None,sensors=None,floats=None,\n              overwrite_index=False,overwrite_profiles=False,skip_download=False,\n              download_individual_profs=False,save_to=None,verbose=True):\n  \"\"\" Downloads GDAC Sprof index file, then selects float profiles based on criteria.\n      Either returns information on profiles and floats (if skip_download=True) or downloads them (if False).\n\n      Arguments:\n          lat_range: None, to select all latitudes\n                     or [lower, upper] within -90 to 90 (selection is inclusive)\n          lon_range: None, to select all longitudes\n                     or [lower, upper] within either -180 to 180 or 0 to 360 (selection is inclusive)\n                     NOTE: longitude range is allowed to cross -180/180 or 0/360\n          start_date: None or datetime object\n          end_date:   None or datetime object\n          sensors: None, to select profiles with any combination of sensors\n                   or string or list of strings to specify required sensors\n                   > note that common options include PRES, TEMP, PSAL, DOXY, CHLA, BBP700,\n                                                      PH_IN_SITU_TOTAL, and NITRATE\n          floats: None, to select any floats matching other criteria\n                  or int or list of ints specifying floats' WMOID numbers\n          overwrite_index: False to keep existing downloaded GDAC index file, or True to download new index\n          overwrite_profiles: False to keep existing downloaded profile files, or True to download new files\n          skip_download: True to skip download and return: (, ,\n                                                            )\n                         or False to download those profiles\n          download_individual_profs: False to download single Sprof file containing all profiles for each float\n                                     or True to download individual profile files for each float\n          save_to: None to download to Google Drive \"/GO-BGC Workshop/Profiles\" directory\n                   or string to specify directory path for profile downloads\n          verbose: True to announce progress, or False to stay silent\n\n  \"\"\"\n  # Paths\n  url_root = 'https://www.usgodae.org/ftp/outgoing/argo/'\n  dac_url_root = url_root + 'dac/'\n  index_filename = 'argo_synthetic-profile_index.txt'\n  if save_to is None: save_to = root\n\n  # Download GDAC synthetic profile index file\n  download_file(url_root,index_filename,overwrite=overwrite_index)\n\n  # Load index file into Pandas DataFrame\n  gdac_index = pd.read_csv(root + index_filename,delimiter=',',header=8,parse_dates=['date','date_update'],\n                          date_parser=lambda x: pd.to_datetime(x,format='%Y%m%d%H%M%S'))\n\n  # Establish time and space criteria\n  if lat_range is None:  lat_range = [-90.0,90.0]\n  if lon_range is None:  lon_range = [-180.0,180.0]\n  elif lon_range[0] > 180 or lon_range[1] > 180:\n    if lon_range[0] > 180: lon_range[0] -= 360\n    if lon_range[1] > 180: lon_range[1] -= 360\n  if start_date is None: start_date = datetime(1900,1,1)\n  if end_date is None:   end_date = datetime(2200,1,1)\n\n  float_wmoid_regexp = r'[a-z]*/[0-9]*/profiles/[A-Z]*([0-9]*)_[0-9]*[A-Z]*.nc'\n  gdac_index['wmoid'] = gdac_index['file'].str.extract(float_wmoid_regexp).astype(int)\n  filepath_main_regexp = '([a-z]*/[0-9]*/)profiles/[A-Z]*[0-9]*_[0-9]*[A-Z]*.nc'\n  gdac_index['filepath_main'] = gdac_index['file'].str.extract(filepath_main_regexp)\n  filepath_regexp = '([a-z]*/[0-9]*/profiles/)[A-Z]*[0-9]*_[0-9]*[A-Z]*.nc'\n  gdac_index['filepath'] = gdac_index['file'].str.extract(filepath_regexp)\n  filename_regexp = '[a-z]*/[0-9]*/profiles/([A-Z]*[0-9]*_[0-9]*[A-Z]*.nc)'\n  gdac_index['filename'] = gdac_index['file'].str.extract(filename_regexp)\n\n  # Subset profiles based on time and space criteria\n  gdac_index_subset = gdac_index.loc[np.logical_and.reduce([gdac_index['latitude'] >= lat_range[0],\n                                                            gdac_index['latitude'] <= lat_range[1],\n                                                            gdac_index['date'] >= start_date,\n                                                            gdac_index['date'] <= end_date]),:]\n  if lon_range[1] >= lon_range[0]:    # range does not cross -180/180 or 0/360\n    gdac_index_subset = gdac_index_subset.loc[np.logical_and(gdac_index_subset['longitude'] >= lon_range[0],\n                                                             gdac_index_subset['longitude'] <= lon_range[1])]\n  elif lon_range[1] < lon_range[0]:   # range crosses -180/180 or 0/360\n    gdac_index_subset = gdac_index_subset.loc[np.logical_or(gdac_index_subset['longitude'] >= lon_range[0],\n                                                            gdac_index_subset['longitude'] <= lon_range[1])]\n\n  # If requested, subset profiles using float WMOID criteria\n  if floats is not None:\n    if type(floats) is not list: floats = [floats]\n    gdac_index_subset = gdac_index_subset.loc[gdac_index_subset['wmoid'].isin(floats),:]\n\n  # If requested, subset profiles using sensor criteria\n  if sensors is not None:\n    if type(sensors) is not list: sensors = [sensors]\n    for sensor in sensors:\n      gdac_index_subset = gdac_index_subset.loc[gdac_index_subset['parameters'].str.contains(sensor),:]\n\n  # Examine subsetted profiles\n  wmoids = gdac_index_subset['wmoid'].unique()\n  wmoid_filepaths = gdac_index_subset['filepath_main'].unique()\n\n  # Just return list of floats and DataFrame with subset of index file, or download each profile\n  if not skip_download:\n    downloaded_filenames = []\n    if download_individual_profs:\n      for p_idx in gdac_index_subset.index:\n        download_file(dac_url_root + gdac_index_subset.loc[p_idx]['filepath'],\n                      gdac_index_subset.loc[p_idx]['filename'],\n                      save_to=save_to,overwrite=overwrite_profiles,verbose=verbose)\n        downloaded_filenames.append(gdac_index_subset.loc[p_idx]['filename'])\n    else:\n      for f_idx, wmoid_filepath in enumerate(wmoid_filepaths):\n        download_file(dac_url_root + wmoid_filepath,str(wmoids[f_idx]) + '_Sprof.nc',\n                      save_to=save_to,overwrite=overwrite_profiles,verbose=verbose)\n        downloaded_filenames.append(str(wmoids[f_idx]) + '_Sprof.nc')\n    return wmoids, gdac_index_subset, downloaded_filenames\n  else:\n    return wmoids, gdac_index_subset\n\n","type":"content","url":"/notebooks/argo-access#id-1-0-go-bgc-toolbox-functions","position":13},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"1.1 Using GDAC function to access Argo subsets","lvl2":"1. Downloading with the GO-BGC Toolbox"},"type":"lvl3","url":"/notebooks/argo-access#id-1-1-using-gdac-function-to-access-argo-subsets","position":14},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"1.1 Using GDAC function to access Argo subsets","lvl2":"1. Downloading with the GO-BGC Toolbox"},"content":"\n\n# Get all floats from chosen period\nlat_bounds = [-70,-45]  # used to be -70 to -30\nlon_bounds = [10,70]    # used to be 10, 60\n\n# Try using more time buffer, 2 years. \nstart_yd = datetime(2017,4,20) # datetime(2019,4,30)  \nend_yd = datetime(2021,7,30) # datetime(2019,7,19)  \n\n# dont download, just get wmoids\nwmoids, gdac_index = argo_gdac(lat_range=lat_bounds,lon_range=lon_bounds,\n                               start_date=start_yd,end_date=end_yd,\n                               sensors=None,floats=None,\n                               overwrite_index=True,overwrite_profiles=False,\n                               skip_download=True,download_individual_profs=False,\n                               save_to=profile_dir,verbose=True)\n\n# download specific float #5906030 \n# wmoids, gdac_index, downloaded_filenames \\\n#                    = argo_gdac(lat_range=None,lon_range=None,\n#                                start_date=None,end_date=None,\n#                                sensors=None,floats=5906030,\n#                                overwrite_index=True,overwrite_profiles=False,\n#                                skip_download=False,download_individual_profs=False,\n#                                save_to=profile_dir,verbose=True)\n\n# DSdict = {}\n# for filename in os.listdir(profile_dir):\n#     if filename.endswith(\".nc\"):\n#         fp = profile_dir + filename\n#         single_dataset = xr.open_dataset(fp, decode_times=False)\n#         DSdict[filename[0:7]] = single_dataset\n# # DSdict['5906030']\n\n","type":"content","url":"/notebooks/argo-access#id-1-1-using-gdac-function-to-access-argo-subsets","position":15},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl2":"2. Using the Argopy Python Package"},"type":"lvl2","url":"/notebooks/argo-access#id-2-using-the-argopy-python-package","position":16},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl2":"2. Using the Argopy Python Package"},"content":"argopy is a python package that facilitates access and manipulation of Argo data from all available data sources. The documentation is available \n\nhere.\n\nThe package allows you to use python to select subsets of Argo data, including data from:\n\na) All available data within a “box” (geospatial area and timeframe)\n\nb) A specific float\n\nc) A specific float profile\n\nThe code here is adapted from the argopy documentation and associated examples.\n\n","type":"content","url":"/notebooks/argo-access#id-2-using-the-argopy-python-package","position":17},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"Imports","lvl2":"2. Using the Argopy Python Package"},"type":"lvl3","url":"/notebooks/argo-access#imports-1","position":18},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"Imports","lvl2":"2. Using the Argopy Python Package"},"content":"\n\nfrom argopy import DataFetcher  # This is the class to work with Argo data\nfrom argopy import ArgoIndex  #  This is the class to work with Argo index\nfrom argopy import ArgoNVSReferenceTables  # This is the class to retrieve data from Argo reference tables\nfrom argopy import ArgoColors  # This is a class with usefull pre-defined colors\nfrom argopy.plot import scatter_map, scatter_plot  # This is a function to easily make maps \n\n# Make a fresh start\nimport argopy\nargopy.reset_options()\nargopy.clear_cache()\nargopy.set_options(cachedir='cache_bgc')\n\n#\nimport numpy as np\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\nimport cmocean\nimport xarray as xr\nxr.set_options(display_expand_attrs = False)\n\nimport logging\nlogging.getLogger(\"matplotlib\").setLevel(logging.ERROR)\nlogging.getLogger(\"pyproj\").setLevel(logging.ERROR)\nlogging.getLogger(\"fsspec\").setLevel(logging.ERROR)\nlogging.getLogger(\"parso\").setLevel(logging.ERROR)\nlogging.getLogger(\"asyncio\").setLevel(logging.ERROR)\nDEBUGFORMATTER = '%(asctime)s [%(levelname)s] [%(name)s] %(filename)s:%(lineno)d: %(message)s'\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format=DEBUGFORMATTER,\n    datefmt='%I:%M:%S %p',\n    handlers=[logging.FileHandler(\"nb-docs.log\", mode='w')]\n)\n\n","type":"content","url":"/notebooks/argo-access#imports-1","position":19},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"a) Fetching data for all profiles within a geographic box","lvl2":"2. Using the Argopy Python Package"},"type":"lvl3","url":"/notebooks/argo-access#a-fetching-data-for-all-profiles-within-a-geographic-box","position":20},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"a) Fetching data for all profiles within a geographic box","lvl2":"2. Using the Argopy Python Package"},"content":"\n\nDefine the geographic region you want to investigate within the BOX variable:\n\n# Format: [lon_min, lon_max, lat_min, lat_max, pres_min, pres_max, datim_min, datim_max]\nBOX = [-56, -45, 54, 60, 0, 2000, '2022-01', '2023-01']\n\n","type":"content","url":"/notebooks/argo-access#a-fetching-data-for-all-profiles-within-a-geographic-box","position":21},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl4":"Retrieve the data:","lvl3":"a) Fetching data for all profiles within a geographic box","lvl2":"2. Using the Argopy Python Package"},"type":"lvl4","url":"/notebooks/argo-access#retrieve-the-data","position":22},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl4":"Retrieve the data:","lvl3":"a) Fetching data for all profiles within a geographic box","lvl2":"2. Using the Argopy Python Package"},"content":"argopy works by constructing a “Fetcher” object, named “f” here. When we define f, we specify the kinds of data we want, and also how we want to process it.\n\nInput arguments:\n\nds: specifies what Argo dataset to retrieve\n\n“phy”: physical Argo data (Temperature, Salinity, Pressure)\n\n“bgc”: biogeochemical data. Note that BGC data can only be retrieved in expert mode (real-time, no QC) as of now (2024-06-13)\n\nmode: specifies the level of data QC you want\n\n“expert”: returns all Argo data. This is raw data with no QC or postprocessing\n\n“standard”: this includes real-time data that has undergone automated QC and is probably good quality, but has not been checked by a human\n\n“resesarch”: this is the most trustworthy data, and only includes delayed mode data that has undergone QC and and been checked by a human expert\n\nparallel: if True, parallelizes the data retrieval process to speed it up\n\nprogress: if True, will display a progress bar of data retrieval\n\ncache: I’m not sure what this does\n\nchunks_maxsize: specifies how to chunk the data request into smaller domains\n\nOnce “f” is defined, we can specify f.region(BOX) and load our data.\n\n","type":"content","url":"/notebooks/argo-access#retrieve-the-data","position":23},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl4":"Construct a fetcher object","lvl3":"a) Fetching data for all profiles within a geographic box","lvl2":"2. Using the Argopy Python Package"},"type":"lvl4","url":"/notebooks/argo-access#construct-a-fetcher-object","position":24},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl4":"Construct a fetcher object","lvl3":"a) Fetching data for all profiles within a geographic box","lvl2":"2. Using the Argopy Python Package"},"content":"\n\n%%time\n# f = DataFetcher(ds='bgc', mode='expert', params='all', parallel=True, progress=True).region(BOX).load()  # Fetch everything !\nf = DataFetcher(ds='phy', mode='research', params='all',\n                parallel=True, progress=True, cache=False,\n                chunks_maxsize={'time': 30},\n               )\nf = f.region(BOX).load()\nf\n\n","type":"content","url":"/notebooks/argo-access#construct-a-fetcher-object","position":25},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl4":"Extract the data from the fetcher object**","lvl3":"a) Fetching data for all profiles within a geographic box","lvl2":"2. Using the Argopy Python Package"},"type":"lvl4","url":"/notebooks/argo-access#extract-the-data-from-the-fetcher-object","position":26},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl4":"Extract the data from the fetcher object**","lvl3":"a) Fetching data for all profiles within a geographic box","lvl2":"2. Using the Argopy Python Package"},"content":"Once the data is loaded, we can \n\nextract our data as an xarray dataset. Using \n\nf.data, the default output is a 1D array of all measurements across all profiles.\n\nds_points = f.data\n\nds_points\n\nConverting to a 2D array of profiles\n\nUsing the \n\ndataset​.argo​.point2profile() method, we can turn the 1d array into a 2D array, grouped by individual profiles.\n\nNote that each N_PROF is unique, although the dataset does not include identifying metadata for the profiles, such as WMO number.\n\nds_profiles = ds_points.argo.point2profile();\n\nds_profiles\n\n","type":"content","url":"/notebooks/argo-access#extract-the-data-from-the-fetcher-object","position":27},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl4":"Extract float metadata from the fetcher object","lvl3":"a) Fetching data for all profiles within a geographic box","lvl2":"2. Using the Argopy Python Package"},"type":"lvl4","url":"/notebooks/argo-access#extract-float-metadata-from-the-fetcher-object","position":28},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl4":"Extract float metadata from the fetcher object","lvl3":"a) Fetching data for all profiles within a geographic box","lvl2":"2. Using the Argopy Python Package"},"content":"Float metadata, including the float’s unique WMO number, and each profile’s cycle number, are retrieved as a pandas dataframe using f.index\n\nf.index\n\n","type":"content","url":"/notebooks/argo-access#extract-float-metadata-from-the-fetcher-object","position":29},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl4":"Basic data visualization","lvl3":"a) Fetching data for all profiles within a geographic box","lvl2":"2. Using the Argopy Python Package"},"type":"lvl4","url":"/notebooks/argo-access#basic-data-visualization","position":30},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl4":"Basic data visualization","lvl3":"a) Fetching data for all profiles within a geographic box","lvl2":"2. Using the Argopy Python Package"},"content":"argopy includes some \n\nbuilt-in data visualization functions\n\nHere is the default map function, which plots each float’s trajectory, colored by the float.\n\nscatter_map(ds_profiles);\n\nWe can also zoom out and see where the data globally\n\nfig, ax = scatter_map(ds_profiles,\n                   figsize=(10,6),\n                   set_global=True,\n                   markersize=2,\n                   markeredgecolor=None,\n                   legend_title='Floats WMO',\n                   cmap='Set2')\n\n","type":"content","url":"/notebooks/argo-access#basic-data-visualization","position":31},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"b) Fetching data for a specific float(s)","lvl2":"2. Using the Argopy Python Package"},"type":"lvl3","url":"/notebooks/argo-access#b-fetching-data-for-a-specific-float-s","position":32},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"b) Fetching data for a specific float(s)","lvl2":"2. Using the Argopy Python Package"},"content":"This works much the same as the example above. However, instead of requesting a BOX with spatiotemporal bounds, we request a specific float by its unique WMO number. For multiple floats, simply pass a list of WMO numbers\n\n%%time\nf = DataFetcher(ds='phy', mode='research', params='all',\n                parallel=True, progress=True, cache=False,\n                chunks_maxsize={'time': 30},\n               )\n\n# We use the f.float() method to fetch data from a specific float (using its WMO#, here 5904673)\n# To request multiple floats, simply pass a list of multiple WMO numbers, e.g. [5904673, 5904672]\nf = f.float(5904673).load();\nf\n\n","type":"content","url":"/notebooks/argo-access#b-fetching-data-for-a-specific-float-s","position":33},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl4":"Extracting and manipulating the data","lvl3":"b) Fetching data for a specific float(s)","lvl2":"2. Using the Argopy Python Package"},"type":"lvl4","url":"/notebooks/argo-access#extracting-and-manipulating-the-data","position":34},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl4":"Extracting and manipulating the data","lvl3":"b) Fetching data for a specific float(s)","lvl2":"2. Using the Argopy Python Package"},"content":"As before, we extract the data as a 1D array of measurements in xarray, and convert that into a 2D array of profiles using ds.argo.point2profile()\n\nds_points = f.data\nds_profiles = ds_points.argo.point2profile();\n\n","type":"content","url":"/notebooks/argo-access#extracting-and-manipulating-the-data","position":35},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl4":"Visualizing the data","lvl3":"b) Fetching data for a specific float(s)","lvl2":"2. Using the Argopy Python Package"},"type":"lvl4","url":"/notebooks/argo-access#visualizing-the-data","position":36},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl4":"Visualizing the data","lvl3":"b) Fetching data for a specific float(s)","lvl2":"2. Using the Argopy Python Package"},"content":"This float is from the Southern Ocean’s Pacific sector\n\nfig, ax = scatter_map(ds_profiles,\n                   figsize=(10,6),\n                   set_global=True,\n                   markersize=2,\n                   markeredgecolor=None,\n                   legend_title='Floats WMO',\n                   cmap='Set2')\n\nNow let’s plot the float’s temperature profiles over its trajectory\n\n# We use xarray's built-in plotting function on the temperature data array\n# We transpose it so that the vertical dimension (N_LEVELS) is on the y-axis\nds_profiles.TEMP.transpose().plot() \nplt.gca().invert_yaxis() # Invert the y-axis so the ocean's surface is at the top\n\n","type":"content","url":"/notebooks/argo-access#visualizing-the-data","position":37},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"c) Fetching data for a specific float profile(s)","lvl2":"2. Using the Argopy Python Package"},"type":"lvl3","url":"/notebooks/argo-access#c-fetching-data-for-a-specific-float-profile-s","position":38},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"c) Fetching data for a specific float profile(s)","lvl2":"2. Using the Argopy Python Package"},"content":"Let’s narrow it down even further, and request a single profile using the fetcher’s f.profile() method, and passing the float’s unique WMO number and the profile number.\nTo request multiple profiles, simply pass a list of profile numbers\n\n%%time\nf = DataFetcher(ds='phy', mode='research', params='all',\n                parallel=True, progress=True, cache=False,\n                chunks_maxsize={'time': 30},\n               )\n\n# We use the f.profile() method to fetch data from a specific profile using the float WMO number and profile number\n# To request multiple profiles, simply pass a list of multiple profile numbers, e.g. (5904673,[1,2,3])\nf = f.profile(5904673,30).load();\nf\n\n","type":"content","url":"/notebooks/argo-access#c-fetching-data-for-a-specific-float-profile-s","position":39},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl4":"Extracting and manipulating the data","lvl3":"c) Fetching data for a specific float profile(s)","lvl2":"2. Using the Argopy Python Package"},"type":"lvl4","url":"/notebooks/argo-access#extracting-and-manipulating-the-data-1","position":40},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl4":"Extracting and manipulating the data","lvl3":"c) Fetching data for a specific float profile(s)","lvl2":"2. Using the Argopy Python Package"},"content":"Once again, we extract the data as a 1D array of measurements in xarray, and convert that into a 2D array of profiles using ds.argo.point2profile()\n\nds_points = f.data\nds_profiles = ds_points.argo.point2profile();\n\n","type":"content","url":"/notebooks/argo-access#extracting-and-manipulating-the-data-1","position":41},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl4":"Visualizing the data","lvl3":"c) Fetching data for a specific float profile(s)","lvl2":"2. Using the Argopy Python Package"},"type":"lvl4","url":"/notebooks/argo-access#visualizing-the-data-1","position":42},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl4":"Visualizing the data","lvl3":"c) Fetching data for a specific float profile(s)","lvl2":"2. Using the Argopy Python Package"},"content":"Let’s plot the vertical temperature and salinity profiles\n\nplt.plot(ds_profiles.sel(N_PROF=0).TEMP.data,ds_profiles.sel(N_PROF=0).PRES.data) # Plot Temperature versus Pressure (i.e. depth)\nplt.gca().invert_yaxis() # Invert the axis to put the surface at the top\nplt.xlabel('Temperature (C)')\nplt.ylabel('Pressure (dbar)')\nplt.title('Temperature profile: Float 5904673, Profile 30')\n\n\nplt.plot(ds_profiles.sel(N_PROF=0).PSAL.data,ds_profiles.sel(N_PROF=0).PRES.data) # Plot Temperature versus Pressure (i.e. depth)\nplt.gca().invert_yaxis() # Invert the axis to put the surface at the top\nplt.xlabel('Practical Salinity (psu)')\nplt.ylabel('Pressure (dbar)')\nplt.title('Salinity profile: Float 5904673, Profile 30')\n\n\n","type":"content","url":"/notebooks/argo-access#visualizing-the-data-1","position":43},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl2":"3. Querying Data with Argovis"},"type":"lvl2","url":"/notebooks/argo-access#id-3-querying-data-with-argovis","position":44},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl2":"3. Querying Data with Argovis"},"content":"\n\nArgovis provides an API that allows us to interact with Argo data while only downloading the exact subsets of data needed for analysis.\nOur examples here are modified from the \n\ntutorial notebooks released by Argovis. We showcase only a few of the functionalities, but more information can be found in the previous link.\n\nThe introduction published by Argovis:\n\n“Argovis is a REST API and web application for searching, downloading, co-locating and visualizing oceanographic data, including Argo array data, ship-based profile data, data from the Global Drifter Program, tropical cyclone data, and several gridded products. Our API is meant to be integrated into living documents like Jupyter notebooks and analyses intended to update their consumption of Argo data in near-real-time, and our web frontend is intended to make it easy for students and educators to explore data about Earth’s oceans at will.”\n\nArgovis should be cited as:\n\nTucker, T., D. Giglio, M. Scanderbeg, and S.S.P. Shen: Argovis: A Web Application for Fast Delivery, Visualization, and Analysis of Argo Data. J. Atmos. Oceanic Technol., 37, 401–416, \n\nTucker et al. (2020)\n\n","type":"content","url":"/notebooks/argo-access#id-3-querying-data-with-argovis","position":45},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"Getting started with argovisHelpers","lvl2":"3. Querying Data with Argovis"},"type":"lvl3","url":"/notebooks/argo-access#getting-started-with-argovishelpers","position":46},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"Getting started with argovisHelpers","lvl2":"3. Querying Data with Argovis"},"content":"\n\nFrom the Argovis tutorial:\n\nIn order to allocate Argovis’s limited computing resources fairly, users are encouraged to register and request a free API key. This works like a password that identifies your requests to Argovis. To do so:\n\nVisit \n\nhttps://​argovis​-keygen​.colorado​.edu/\n\nFill out the form under New Account Registration\n\nAn API key will be emailed to you shortly.\n\nTreat this API key like a password - don’t share it or leave it anywhere public. If you ever forget it or accidentally reveal it to a third party, see the same website above to change or deactivate your token.\n\nPut your API key in the quotes in the variable below before moving on:\n\nAPI_ROOT='https://argovis-api.colorado.edu/'\nAPI_KEY='de6ee72a54bc5ca29dee5c801cab13fa4a354985'\n\n","type":"content","url":"/notebooks/argo-access#getting-started-with-argovishelpers","position":47},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"Getting Argo data documents","lvl2":"3. Querying Data with Argovis"},"type":"lvl3","url":"/notebooks/argo-access#getting-argo-data-documents","position":48},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"Getting Argo data documents","lvl2":"3. Querying Data with Argovis"},"content":"\n\nBefore actually getting Argo measurements, we can query information about the profile (including pointers to the metadata).\n\nargoSearch = {\n    'startDate': '2013-05-01T00:00:00Z',\n    'endDate': '2023-05-01T00:00:00Z',\n    'center': '-22.5,0',\n    'radius': 100\n}\n\nargoProfiles = avh.query('argo', options=argoSearch, apikey=API_KEY, apiroot=API_ROOT)\nargoProfiles[0]\n\nargoProfiles[0]['_id']\n\nNote that the first object in argoProfiles is a single vertical Argo “profile”.\nThe first 7 digits of argoProfiles[0]['_id'] refer to a float’s WMO unique identification number.\nThe last three digits are the profile number.\n\nIn the above example, we are looking at data from the 256th profile from float WMO #1901820.\n\nWe can get more information about this particular float by querying argo/meta.\n\nmetaOptions = {\n    'id': argoProfiles[0]['metadata'][0]\n}\nargoMeta = avh.query('argo/meta', options=metaOptions, apikey=API_KEY, apiroot=API_ROOT)\nargoMeta\n\nWe can also specify all of the profiles taken from the same float with WMO ID 1901820.\n\nplatformSearch = {\n    'platform': argoMeta[0]['platform']\n}\n\nplatformProfiles = avh.query('argo', options=platformSearch, apikey=API_KEY, apiroot=API_ROOT)\nprint(len(platformProfiles))\n\n","type":"content","url":"/notebooks/argo-access#getting-argo-data-documents","position":49},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"Making data queries","lvl2":"3. Querying Data with Argovis"},"type":"lvl3","url":"/notebooks/argo-access#making-data-queries","position":50},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"Making data queries","lvl2":"3. Querying Data with Argovis"},"content":"\n\nNow, we want to retrieve actual measurements. We can use any number of identifiers.\n\nBelow, we are specifying float WMO 4901283 and profile #003. The data variable can be:\n\nA comma separated list of variable names, e.g. 'temperature, doxy'\n\n'all', meaning get all available variables.\n\ndataQuery = {\n    'id': '4901283_003',\n    'data': 'all'\n}\nprofile = avh.query('argo', options=dataQuery, apikey=API_KEY, apiroot=API_ROOT)\n# avh.data_inflate(profile[0])[0:10]\n\nWe can query float profiles within larger bounds:\n\ndataQuery = {\n    'startDate': '2020-01-01T00:00:00Z',\n    'endDate': '2024-01-01T00:00:00Z',\n    'polygon': [[-150,-30],[-155,-30],[-155,-35],[-150,-35],[-150,-30]],\n    'data': 'doxy'\n}\n\nprofiles = avh.query('argo', options=dataQuery, apikey=API_KEY, apiroot=API_ROOT)\n\ninflated_data = avh.data_inflate(profiles[0])\ninflated_data[0:10]\n\n","type":"content","url":"/notebooks/argo-access#making-data-queries","position":51},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"Querying within geospatial bounds","lvl2":"3. Querying Data with Argovis"},"type":"lvl3","url":"/notebooks/argo-access#querying-within-geospatial-bounds","position":52},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"Querying within geospatial bounds","lvl2":"3. Querying Data with Argovis"},"content":"\n\nqs = {\n    'startDate': '2017-08-01T00:00:00Z',\n    'endDate': '2017-09-01T00:00:00Z',\n    'box': [[-20,70],[20,72]]\n}\n\nprofiles = avh.query('argo', options=qs, apikey=API_KEY, apiroot=API_ROOT)\nlatitudes = [x['geolocation']['coordinates'][1] for x in profiles]\nprint(min(latitudes))\nprint(max(latitudes))\n\n\n\n","type":"content","url":"/notebooks/argo-access#querying-within-geospatial-bounds","position":53},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"Subsection to the second section","lvl2":"3. Querying Data with Argovis"},"type":"lvl3","url":"/notebooks/argo-access#subsection-to-the-second-section","position":54},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"Subsection to the second section","lvl2":"3. Querying Data with Argovis"},"content":"","type":"content","url":"/notebooks/argo-access#subsection-to-the-second-section","position":55},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl4":"a quick demonstration","lvl3":"Subsection to the second section","lvl2":"3. Querying Data with Argovis"},"type":"lvl4","url":"/notebooks/argo-access#a-quick-demonstration","position":56},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl4":"a quick demonstration","lvl3":"Subsection to the second section","lvl2":"3. Querying Data with Argovis"},"content":"","type":"content","url":"/notebooks/argo-access#a-quick-demonstration","position":57},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl5":"of further and further","lvl4":"a quick demonstration","lvl3":"Subsection to the second section","lvl2":"3. Querying Data with Argovis"},"type":"lvl5","url":"/notebooks/argo-access#of-further-and-further","position":58},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl5":"of further and further","lvl4":"a quick demonstration","lvl3":"Subsection to the second section","lvl2":"3. Querying Data with Argovis"},"content":"","type":"content","url":"/notebooks/argo-access#of-further-and-further","position":59},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl6":"header levels","lvl5":"of further and further","lvl4":"a quick demonstration","lvl3":"Subsection to the second section","lvl2":"3. Querying Data with Argovis"},"type":"lvl6","url":"/notebooks/argo-access#header-levels","position":60},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl6":"header levels","lvl5":"of further and further","lvl4":"a quick demonstration","lvl3":"Subsection to the second section","lvl2":"3. Querying Data with Argovis"},"content":"\n\nas well m = a * t / h text! Similarly, you have access to other \\LaTeX equation \n\nfunctionality via MathJax (demo below from link),\\begin{align}\n\\dot{x} & = \\sigma(y-x) \\\\\n\\dot{y} & = \\rho x - y - xz \\\\\n\\dot{z} & = -\\beta z + xy\n\\end{align}\n\nCheck out \n\nany number of helpful Markdown resources for further customizing your notebooks and the \n\nJupyter docs for Jupyter-specific formatting information. Don’t hesitate to ask questions if you have problems getting it to look just right.\n\n","type":"content","url":"/notebooks/argo-access#header-levels","position":61},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl2":"Last Section"},"type":"lvl2","url":"/notebooks/argo-access#last-section","position":62},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl2":"Last Section"},"content":"If you’re comfortable, and as we briefly used for our embedded logo up top, you can embed raw html into Jupyter Markdown cells (edit to see):\n\nInfoYour relevant information here!\n\nFeel free to copy this around and edit or play around with yourself. Some other admonitions you can put in:\n\nSuccessWe got this done after all!\n\nWarningBe careful!\n\nDangerScary stuff be here.\n\nWe also suggest checking out Jupyter Book’s \n\nbrief demonstration on adding cell tags to your cells in Jupyter Notebook, Lab, or manually. Using these cell tags can allow you to \n\ncustomize how your code content is displayed and even \n\ndemonstrate errors without altogether crashing our loyal army of machines!\n\n\n\n","type":"content","url":"/notebooks/argo-access#last-section","position":63},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/argo-access#summary","position":64},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl2":"Summary"},"content":"Add one final --- marking the end of your body of content, and then conclude with a brief single paragraph summarizing at a high level the key pieces that were learned and how they tied to your objectives. Look to reiterate what the most important takeaways were.","type":"content","url":"/notebooks/argo-access#summary","position":65},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/argo-access#whats-next","position":66},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl3":"What’s next?","lvl2":"Summary"},"content":"Let Jupyter book tie this to the next (sequential) piece of content that people could move on to down below and in the sidebar. However, if this page uniquely enables your reader to tackle other nonsequential concepts throughout this book, or even external content, link to it here!\n\n","type":"content","url":"/notebooks/argo-access#whats-next","position":67},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/argo-access#resources-and-references","position":68},{"hierarchy":{"lvl1":"Accessing Argo Data","lvl2":"Resources and references"},"content":"Finally, be rigorous in your citations and references as necessary. Give credit where credit is due. Also, feel free to link to relevant external material, further reading, documentation, etc. Then you’re done! Give yourself a quick review, a high five, and send us a pull request. A few final notes:\n\nKernel > Restart Kernel and Run All Cells... to confirm that your notebook will cleanly run from start to finish\n\nKernel > Restart Kernel and Clear All Outputs... before committing your notebook, our machines will do the heavy lifting\n\nTake credit! Provide author contact information if you’d like; if so, consider adding information here at the bottom of your notebook\n\nGive credit! Attribute appropriate authorship for referenced code, information, images, etc.\n\nOnly include what you’re legally allowed: no copyright infringement or plagiarism\n\nThank you for your contribution!","type":"content","url":"/notebooks/argo-access#resources-and-references","position":69},{"hierarchy":{"lvl1":"Introduction to Argo Observations"},"type":"lvl1","url":"/notebooks/argo-introduction","position":0},{"hierarchy":{"lvl1":"Introduction to Argo Observations"},"content":"\n\n","type":"content","url":"/notebooks/argo-introduction","position":1},{"hierarchy":{"lvl1":"Introduction to Argo Observations"},"type":"lvl1","url":"/notebooks/argo-introduction#introduction-to-argo-observations","position":2},{"hierarchy":{"lvl1":"Introduction to Argo Observations"},"content":"\n\n\n\n","type":"content","url":"/notebooks/argo-introduction#introduction-to-argo-observations","position":3},{"hierarchy":{"lvl1":"Introduction to Argo Observations","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/argo-introduction#overview","position":4},{"hierarchy":{"lvl1":"Introduction to Argo Observations","lvl2":"Overview"},"content":"Argo floats are autonomous profiling instruments that sample the interior ocean by traveling with the ocean currents. They are part of the Argo program, an international collaborative effort to monitor the state of the ocean. The deployment of thousands of Argo floats has enabled new observation-based studies in previously undersampled regions (e.g. Wong et al. 2020, Swart et al. 2023). Some newer Argo floats, known as Biogeochemical-Argo (BGC-Argo) floats, have been further equipped with biologically relevant sensors for oxygen, nitrate, optical backscatter, and chlorophyll fluorescence (Claustre et al. 2020, Sarmiento et al. 2023, Roemmich et al. 2019). Argo floats provide critical data that enhances our understanding of the ocean and its influence on the global climate system. Through international collaboration and advanced technology, the Argo program plays a vital role in monitoring and studying the world’s oceans.\n\nHere, we introduce Argo profiling floats, which are autonomous instruments that operate remotely and sample the ocean interior continuously.\nOur objectives:\n\nWhat are Argo floats?\n\nHow are the data formatted?\n\nHow can we transform data to work with?\n\nWhat are some ways of visualizing float data?\n\nIn the next notebook, \n\nAccessing Argo Data, we will explore different ways of downloading and retrieving float profiles.\n\n","type":"content","url":"/notebooks/argo-introduction#overview","position":5},{"hierarchy":{"lvl1":"Introduction to Argo Observations","lvl3":"Basic profiling scheme of an Argo float","lvl2":"Overview"},"type":"lvl3","url":"/notebooks/argo-introduction#basic-profiling-scheme-of-an-argo-float","position":6},{"hierarchy":{"lvl1":"Introduction to Argo Observations","lvl3":"Basic profiling scheme of an Argo float","lvl2":"Overview"},"content":"\n\nfrom IPython.display import Image\nprint('Float Cycle Diagram, courtesy of UCSD (https://argo.ucsd.edu/how-do-floats-work/) ')\nImage(filename='../images/float_cycle_diagram.png') \n\n\n","type":"content","url":"/notebooks/argo-introduction#basic-profiling-scheme-of-an-argo-float","position":7},{"hierarchy":{"lvl1":"Introduction to Argo Observations","lvl2":"Prerequisites (fill in)"},"type":"lvl2","url":"/notebooks/argo-introduction#prerequisites-fill-in","position":8},{"hierarchy":{"lvl1":"Introduction to Argo Observations","lvl2":"Prerequisites (fill in)"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to NetCDF\n\nNecessary\n\nFamiliarity with metadata structure\n\nIntro to Xarray\n\nNecessary\n\n\n\nIntro to Matplotlib\n\nHelpful\n\n\n\nTime to learn: 15 min\n\n\n\n","type":"content","url":"/notebooks/argo-introduction#prerequisites-fill-in","position":9},{"hierarchy":{"lvl1":"Introduction to Argo Observations","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/argo-introduction#imports","position":10},{"hierarchy":{"lvl1":"Introduction to Argo Observations","lvl2":"Imports"},"content":"Begin your body of content with another --- divider before continuing into this section, then remove this body text and populate the following code cell with all necessary Python imports up-front:\n\n# Import packages\nimport sys\nimport os\nimport numpy as np\nimport pandas as pd\nimport scipy\nimport xarray as xr\nfrom datetime import datetime, timedelta\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport seaborn as sns\nfrom cmocean import cm as cmo\n\n","type":"content","url":"/notebooks/argo-introduction#imports","position":11},{"hierarchy":{"lvl1":"Introduction to Argo Observations","lvl2":"How is Argo data formatted?"},"type":"lvl2","url":"/notebooks/argo-introduction#how-is-argo-data-formatted","position":12},{"hierarchy":{"lvl1":"Introduction to Argo Observations","lvl2":"How is Argo data formatted?"},"content":"\n\nIn the next notebook, we will discuss other ways of accessing Argo data.\nHere, we will use one float as an example for what the platform is observing.\n\nOften, floats are packaged into xarray Datasets, which are objects that can deal with data with multiple dimensions.\n\nwmo = 5905367\nflt_data = xr.open_dataset('../data/' + str(wmo) + '_Sprof.nc', decode_times=False)\nflt_data\n\nNote that the “Attributes” of the .nc file show metadata on the platform.\nWe can also look at more specific attributes of variables.\n\nMost users should always use the *_ADJUSTED values. The corresponding variable *_ADJUSTED_QC gives the standard Argo QC flags.\n\nflt_data.TEMP_ADJUSTED\n\n","type":"content","url":"/notebooks/argo-introduction#how-is-argo-data-formatted","position":13},{"hierarchy":{"lvl1":"Introduction to Argo Observations","lvl3":"Plotting time-depth sections from a single float.","lvl2":"How is Argo data formatted?"},"type":"lvl3","url":"/notebooks/argo-introduction#plotting-time-depth-sections-from-a-single-float","position":14},{"hierarchy":{"lvl1":"Introduction to Argo Observations","lvl3":"Plotting time-depth sections from a single float.","lvl2":"How is Argo data formatted?"},"content":"\n\nfig = flt_data.TEMP.T.plot(figsize = (6,3), cmap = cmo.thermal)\nax = plt.gca()\nax.invert_yaxis()\nax.set_title('Adjusted Temperature from WMO ' + str(wmo))\n\n","type":"content","url":"/notebooks/argo-introduction#plotting-time-depth-sections-from-a-single-float","position":15},{"hierarchy":{"lvl1":"Introduction to Argo Observations","lvl3":"Transforming an Xarray Dataset into a Pandas Dataframe.","lvl2":"How is Argo data formatted?"},"type":"lvl3","url":"/notebooks/argo-introduction#transforming-an-xarray-dataset-into-a-pandas-dataframe","position":16},{"hierarchy":{"lvl1":"Introduction to Argo Observations","lvl3":"Transforming an Xarray Dataset into a Pandas Dataframe.","lvl2":"How is Argo data formatted?"},"content":"\n\nThe pd Dataframe is a useful object resembling a table (can be interchangeable with .csv files).\n\nAfter transforming to a Dataframe, we can easily pull variables of interest, and add others.\n\nNote that having time in a numerical format (here, “yearday” since 2019-01-01) helps with training models.\n\n# Choose variables to use\nfloat_df = flt_data[['JULD','LATITUDE', 'LONGITUDE','PRES_ADJUSTED','TEMP_ADJUSTED','PSAL_ADJUSTED',\n                         'DOXY_ADJUSTED','NITRATE_ADJUSTED', # , 'PH_IN_SITU_TOTAL_ADJUSTED','BBP700_ADJUSTED', \n                        'JULD_QC', 'POSITION_QC', 'PRES_ADJUSTED_QC', 'TEMP_ADJUSTED_QC','PSAL_ADJUSTED_QC', \n                        'DOXY_ADJUSTED_QC', 'NITRATE_ADJUSTED_QC']].to_dataframe() #'BBP700_ADJUSTED_QC' #, 'PH_IN_SITU_TOTAL_ADJUSTED_QC',\ndtimes = pd.to_datetime(float_df.JULD.values, unit='D', origin=pd.Timestamp('1950-01-01'))\n\ndef datetime2ytd(time):\n    \"\"\" Return time in YTD format from datetime format.\n    Sometimes easier to work with numerical date values.\"\"\"\n    return (time - np.datetime64('2019-01-01'))/np.timedelta64(1, 'D')\nfloat_df['YEARDAY'] = datetime2ytd(dtimes)\n\n# Make a profile ID for each profile\nprof = float_df.index.get_level_values(0)\nprof = prof.astype(str); prof = [tag.zfill(3) for tag in prof]\nfloat_df['PROFID'] = [str(wmo)+tag for tag in prof]\n\n\n# Convert QC flags to integers\nqc_keys = ['JULD_QC', 'POSITION_QC', 'PRES_ADJUSTED_QC', 'TEMP_ADJUSTED_QC', 'PSAL_ADJUSTED_QC',\n    'DOXY_ADJUSTED_QC', 'NITRATE_ADJUSTED_QC'] #, 'PH_IN_SITU_TOTAL_ADJUSTED_QC', 'BBP700_ADJUSTED_QC']\nfor key in qc_keys:  #qc flags are not stored as ints so we can convert\n        newlist = []\n        for qc in float_df[key]:\n                if str(qc)[2] == 'n': newlist.append('NaN')\n                else: newlist.append(str(qc)[2])\n        float_df[key] = newlist\n\n\nHere is what the resulting dataframe might look like:\n\nfloat_df\n\n","type":"content","url":"/notebooks/argo-introduction#transforming-an-xarray-dataset-into-a-pandas-dataframe","position":17},{"hierarchy":{"lvl1":"Introduction to Argo Observations","lvl3":"Considering dive-averaged data","lvl2":"How is Argo data formatted?"},"type":"lvl3","url":"/notebooks/argo-introduction#considering-dive-averaged-data","position":18},{"hierarchy":{"lvl1":"Introduction to Argo Observations","lvl3":"Considering dive-averaged data","lvl2":"How is Argo data formatted?"},"content":"\n\nThe sampling strategy of the float is to sample as it ascends from a dive.\nIf we want to know the average information for the entire dive, we can average over the dive observations.\n\ndef list_profile_DFs(df):\n    \"\"\" \n    @param df: dataframe with all profiles\n    @return: list of dataframes, each with a unique profile\n    \"\"\"\n    PROFIDs = pd.unique(df.PROFID)\n    profile_DFs = []\n    for i in range(len(PROFIDs)):\n        profile_DFs.append(df[df['PROFID']==PROFIDs[i]].copy())\n    return profile_DFs\n\n\ndef make_diveav(df):\n    \"\"\"\n    Make dive-averaged dataframes (per profile). \n    @param:    df: dataframe with all profiles\n    @return:   single dataframe with dive-averaged data\n    \"\"\"\n    prof_list = list_profile_DFs(df)\n\n    newDF = pd.DataFrame()\n    newDF['PROFID'] = pd.unique(df.PROFID)\n    newDF['YEARDAY'] = [np.nanmean(x.YEARDAY) for x in prof_list]\n    newDF['LATITUDE'] = [x.LATITUDE.mean() for x in prof_list]\n    newDF['LONGITUDE'] = [x.LONGITUDE.mean() for x in prof_list]\n\n    return newDF\n\n\n\nThis particular float returns 23 profiles in the bounds we provided earlier.\n\nfloat_dav = make_diveav(float_df)\nfloat_dav\n\nUsing the dive-averaged data, we can easily plot the overall trajectory using matplotlib.\n\n# plot with float trajectory\n\nfig = plt.figure(figsize=(10,4))\nax = fig.gca()\n\nsca = ax.scatter(float_dav.LONGITUDE, float_dav.LATITUDE, c=float_dav.YEARDAY, cmap='viridis', s=20, zorder=3)\nax.plot(float_dav.LONGITUDE, float_dav.LATITUDE, zorder=1, linewidth=0.5, linestyle='dashed', color='k')\nplt.colorbar(sca, shrink=0.6, label='Days since 2019-01-01')\n\nax.yaxis.set_major_formatter(\"{x:1.0f}°S\")\nax.xaxis.set_major_formatter(\"{x:1.0f}°E\")\nax.set_aspect('equal')\nax.grid(zorder=1)\nax.set_ylim([-68, -60])\n\nax.set_title('Float ' + str(wmo) + ' Trajectory in 2019')\n\nIn the next notebook, \n\nAccessing Argo, we go over more details on accessing the Argo data and plotting profiles.\n\n\n\n","type":"content","url":"/notebooks/argo-introduction#considering-dive-averaged-data","position":19},{"hierarchy":{"lvl1":"Introduction to Argo Observations","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/argo-introduction#summary","position":20},{"hierarchy":{"lvl1":"Introduction to Argo Observations","lvl2":"Summary"},"content":"Add one final --- marking the end of your body of content, and then conclude with a brief single paragraph summarizing at a high level the key pieces that were learned and how they tied to your objectives. Look to reiterate what the most important takeaways were.","type":"content","url":"/notebooks/argo-introduction#summary","position":21},{"hierarchy":{"lvl1":"Introduction to Argo Observations","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/argo-introduction#whats-next","position":22},{"hierarchy":{"lvl1":"Introduction to Argo Observations","lvl3":"What’s next?","lvl2":"Summary"},"content":"Let Jupyter book tie this to the next (sequential) piece of content that people could move on to down below and in the sidebar. However, if this page uniquely enables your reader to tackle other nonsequential concepts throughout this book, or even external content, link to it here!\n\n","type":"content","url":"/notebooks/argo-introduction#whats-next","position":23},{"hierarchy":{"lvl1":"Introduction to Argo Observations","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/argo-introduction#resources-and-references","position":24},{"hierarchy":{"lvl1":"Introduction to Argo Observations","lvl2":"Resources and references"},"content":"Finally, be rigorous in your citations and references as necessary. Give credit where credit is due. Also, feel free to link to relevant external material, further reading, documentation, etc. Then you’re done! Give yourself a quick review, a high five, and send us a pull request. A few final notes:\n\nKernel > Restart Kernel and Run All Cells... to confirm that your notebook will cleanly run from start to finish\n\nKernel > Restart Kernel and Clear All Outputs... before committing your notebook, our machines will do the heavy lifting\n\nTake credit! Provide author contact information if you’d like; if so, consider adding information here at the bottom of your notebook\n\nGive credit! Attribute appropriate authorship for referenced code, information, images, etc.\n\nOnly include what you’re legally allowed: no copyright infringement or plagiarism\n\nThank you for your contribution!","type":"content","url":"/notebooks/argo-introduction#resources-and-references","position":25},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in this Project Pythia Cookbook, ‘sklearn-argo’, is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/notebooks/how-to-cite","position":1},{"hierarchy":{"lvl1":"Project Pythia Notebook Template"},"type":"lvl1","url":"/notebooks/sklearn-clustering","position":0},{"hierarchy":{"lvl1":"Project Pythia Notebook Template"},"content":"\n\n","type":"content","url":"/notebooks/sklearn-clustering","position":1},{"hierarchy":{"lvl1":"Project Pythia Notebook Template"},"type":"lvl1","url":"/notebooks/sklearn-clustering#project-pythia-notebook-template","position":2},{"hierarchy":{"lvl1":"Project Pythia Notebook Template"},"content":"Next, title your notebook appropriately with a top-level Markdown header, #. Do not use this level header anywhere else in the notebook. Our book build process will use this title in the navbar, table of contents, etc. Keep it short, keep it descriptive. Follow this with a --- cell to visually distinguish the transition to the prerequisites section.\n\n\n\n","type":"content","url":"/notebooks/sklearn-clustering#project-pythia-notebook-template","position":3},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/sklearn-clustering#overview","position":4},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl2":"Overview"},"content":"If you have an introductory paragraph, lead with it here! Keep it short and tied to your material, then be sure to continue into the required list of topics below,\n\nThis is a numbered list of the specific topics\n\nThese should map approximately to your main sections of content\n\nOr each second-level, ##, header in your notebook\n\nKeep the size and scope of your notebook in check\n\nAnd be sure to let the reader know up front the important concepts they’ll be leaving with\n\n","type":"content","url":"/notebooks/sklearn-clustering#overview","position":5},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/sklearn-clustering#prerequisites","position":6},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl2":"Prerequisites"},"content":"This section was inspired by \n\nthis template of the wonderful \n\nThe Turing Way Jupyter Book.\n\nFollowing your overview, tell your reader what concepts, packages, or other background information they’ll need before learning your material. Tie this explicitly with links to other pages here in Foundations or to relevant external resources. Remove this body text, then populate the Markdown table, denoted in this cell with | vertical brackets, below, and fill out the information following. In this table, lay out prerequisite concepts by explicitly linking to other Foundations material or external resources, or describe generally helpful concepts.\n\nLabel the importance of each concept explicitly as helpful/necessary.\n\nConcepts\n\nImportance\n\nNotes\n\nIntro to Cartopy\n\nNecessary\n\n\n\nUnderstanding of NetCDF\n\nHelpful\n\nFamiliarity with metadata structure\n\nProject management\n\nHelpful\n\n\n\nTime to learn: estimate in minutes. For a rough idea, use 5 mins per subsection, 10 if longer; add these up for a total. Safer to round up and overestimate.\n\nSystem requirements:\n\nPopulate with any system, version, or non-Python software requirements if necessary\n\nOtherwise use the concepts table above and the Imports section below to describe required packages as necessary\n\nIf no extra requirements, remove the System requirements point altogether\n\n\n\n","type":"content","url":"/notebooks/sklearn-clustering#prerequisites","position":7},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/sklearn-clustering#imports","position":8},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl2":"Imports"},"content":"Begin your body of content with another --- divider before continuing into this section, then remove this body text and populate the following code cell with all necessary Python imports up-front:\n\n# import sys\n# import glob\n# import pandas as pd\n# import numpy as np\n\n# from argopy import DataFetcher  # This is the class to work with Argo data\n# from argopy import ArgoIndex  #  This is the class to work with Argo index\n# from argopy import ArgoNVSReferenceTables  # This is the class to retrieve data from Argo reference tables\n# from argopy import ArgoColors  # This is a class with usefull pre-defined colors\n# from argopy.plot import scatter_map, scatter_plot  # This is a function to easily make maps \n\n# # Make a fresh start\n# import argopy\n# argopy.reset_options()\n# argopy.clear_cache()\n# argopy.set_options(cachedir='cache_bgc')\n\n# #\n# import matplotlib as mpl\n# from matplotlib import pyplot as plt\n# import cmocean\n# import xarray as xr\n# xr.set_options(display_expand_attrs = False)\n\n","type":"content","url":"/notebooks/sklearn-clustering#imports","position":9},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl2":"Your first content section"},"type":"lvl2","url":"/notebooks/sklearn-clustering#your-first-content-section","position":10},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl2":"Your first content section"},"content":"\n\nThis is where you begin your first section of material, loosely tied to your objectives stated up front. Tie together your notebook as a narrative, with interspersed Markdown text, images, and more as necessary,\n\n# import logging\n# logging.getLogger(\"matplotlib\").setLevel(logging.ERROR)\n# logging.getLogger(\"pyproj\").setLevel(logging.ERROR)\n# logging.getLogger(\"fsspec\").setLevel(logging.ERROR)\n# logging.getLogger(\"parso\").setLevel(logging.ERROR)\n# logging.getLogger(\"asyncio\").setLevel(logging.ERROR)\n# DEBUGFORMATTER = '%(asctime)s [%(levelname)s] [%(name)s] %(filename)s:%(lineno)d: %(message)s'\n# logging.basicConfig(\n#     level=logging.DEBUG,\n#     format=DEBUGFORMATTER,\n#     datefmt='%I:%M:%S %p',\n#     handlers=[logging.FileHandler(\"nb-docs.log\", mode='w')]\n# )\n\n","type":"content","url":"/notebooks/sklearn-clustering#your-first-content-section","position":11},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl2":"Load a dataset using argopy"},"type":"lvl2","url":"/notebooks/sklearn-clustering#load-a-dataset-using-argopy","position":12},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl2":"Load a dataset using argopy"},"content":"\n\n","type":"content","url":"/notebooks/sklearn-clustering#load-a-dataset-using-argopy","position":13},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl3":"Working off the argopy tutorial","lvl2":"Load a dataset using argopy"},"type":"lvl3","url":"/notebooks/sklearn-clustering#working-off-the-argopy-tutorial","position":14},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl3":"Working off the argopy tutorial","lvl2":"Load a dataset using argopy"},"content":"https://​nbviewer​.org​/github​/euroargodev​/argopy​/blob​/master​/docs​/examples​/BGC​_region​_float​_data​.ipynb\n\n# # Format: [lon_min, lon_max, lat_min, lat_max, pres_min, pres_max, datim_min, datim_max]\n# BOX = [-56, -45, 54, 60, 0, 2000, '2022-01', '2023-01']\n# # BOX = [-56, -45, 54, 60, 0, 2000, '2022-09', '2023-01']\n# BOX = [-56, -45, 54, 60, 0, 500, '2019-01', '2023-01']\n# # BOX = [-75, -62, 38, 42, 0, 2000, '2021-01', '2022-01']\n\n# # Select profile in a space/time domain:\n# index_BOX = [BOX[ii] for ii in [0, 1, 2, 3, 6, 7]]  # We don't want the pressure axis BOX limits\n# idx.search_lat_lon_tim(index_BOX)\n\n# # Get the list of all parameters for this region:\n# idx.read_params()\n\n# # How many different floats in the region:\n# len(idx.read_wmo())\n\n# %%time\n# f = DataFetcher(ds='bgc', mode='expert', params='all',\n#                 parallel=True, progress=True, cache=False,\n#                 chunks_maxsize={'time': 30},\n#                )\n# f = f.region(BOX).load()\n# f\n\n","type":"content","url":"/notebooks/sklearn-clustering#working-off-the-argopy-tutorial","position":15},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl3":"A content subsection","lvl2":"Load a dataset using argopy"},"type":"lvl3","url":"/notebooks/sklearn-clustering#a-content-subsection","position":16},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl3":"A content subsection","lvl2":"Load a dataset using argopy"},"content":"Divide and conquer your objectives with Markdown subsections, which will populate the helpful navbar in Jupyter Lab and here on the Jupyter Book!\n\n# some subsection code\nnew = \"helpful information\"\n\n","type":"content","url":"/notebooks/sklearn-clustering#a-content-subsection","position":17},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl3":"Another content subsection","lvl2":"Load a dataset using argopy"},"type":"lvl3","url":"/notebooks/sklearn-clustering#another-content-subsection","position":18},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl3":"Another content subsection","lvl2":"Load a dataset using argopy"},"content":"Keep up the good work! A note, try to avoid using code comments as narrative, and instead let them only exist as brief clarifications where necessary.\n\n","type":"content","url":"/notebooks/sklearn-clustering#another-content-subsection","position":19},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl2":"Your second content section"},"type":"lvl2","url":"/notebooks/sklearn-clustering#your-second-content-section","position":20},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl2":"Your second content section"},"content":"Here we can move on to our second objective, and we can demonstrate\n\n","type":"content","url":"/notebooks/sklearn-clustering#your-second-content-section","position":21},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl3":"Subsection to the second section","lvl2":"Your second content section"},"type":"lvl3","url":"/notebooks/sklearn-clustering#subsection-to-the-second-section","position":22},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl3":"Subsection to the second section","lvl2":"Your second content section"},"content":"","type":"content","url":"/notebooks/sklearn-clustering#subsection-to-the-second-section","position":23},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl4":"a quick demonstration","lvl3":"Subsection to the second section","lvl2":"Your second content section"},"type":"lvl4","url":"/notebooks/sklearn-clustering#a-quick-demonstration","position":24},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl4":"a quick demonstration","lvl3":"Subsection to the second section","lvl2":"Your second content section"},"content":"","type":"content","url":"/notebooks/sklearn-clustering#a-quick-demonstration","position":25},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl5":"of further and further","lvl4":"a quick demonstration","lvl3":"Subsection to the second section","lvl2":"Your second content section"},"type":"lvl5","url":"/notebooks/sklearn-clustering#of-further-and-further","position":26},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl5":"of further and further","lvl4":"a quick demonstration","lvl3":"Subsection to the second section","lvl2":"Your second content section"},"content":"","type":"content","url":"/notebooks/sklearn-clustering#of-further-and-further","position":27},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl6":"header levels","lvl5":"of further and further","lvl4":"a quick demonstration","lvl3":"Subsection to the second section","lvl2":"Your second content section"},"type":"lvl6","url":"/notebooks/sklearn-clustering#header-levels","position":28},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl6":"header levels","lvl5":"of further and further","lvl4":"a quick demonstration","lvl3":"Subsection to the second section","lvl2":"Your second content section"},"content":"\n\nas well m = a * t / h text! Similarly, you have access to other \\LaTeX equation \n\nfunctionality via MathJax (demo below from link),\\begin{align}\n\\dot{x} & = \\sigma(y-x) \\\\\n\\dot{y} & = \\rho x - y - xz \\\\\n\\dot{z} & = -\\beta z + xy\n\\end{align}\n\nCheck out \n\nany number of helpful Markdown resources for further customizing your notebooks and the \n\nJupyter docs for Jupyter-specific formatting information. Don’t hesitate to ask questions if you have problems getting it to look just right.\n\n","type":"content","url":"/notebooks/sklearn-clustering#header-levels","position":29},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl2":"Last Section"},"type":"lvl2","url":"/notebooks/sklearn-clustering#last-section","position":30},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl2":"Last Section"},"content":"If you’re comfortable, and as we briefly used for our embedded logo up top, you can embed raw html into Jupyter Markdown cells (edit to see):\n\nInfoYour relevant information here!\n\nFeel free to copy this around and edit or play around with yourself. Some other admonitions you can put in:\n\nSuccessWe got this done after all!\n\nWarningBe careful!\n\nDangerScary stuff be here.\n\nWe also suggest checking out Jupyter Book’s \n\nbrief demonstration on adding cell tags to your cells in Jupyter Notebook, Lab, or manually. Using these cell tags can allow you to \n\ncustomize how your code content is displayed and even \n\ndemonstrate errors without altogether crashing our loyal army of machines!\n\n\n\n","type":"content","url":"/notebooks/sklearn-clustering#last-section","position":31},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/sklearn-clustering#summary","position":32},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl2":"Summary"},"content":"Add one final --- marking the end of your body of content, and then conclude with a brief single paragraph summarizing at a high level the key pieces that were learned and how they tied to your objectives. Look to reiterate what the most important takeaways were.","type":"content","url":"/notebooks/sklearn-clustering#summary","position":33},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/sklearn-clustering#whats-next","position":34},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl3":"What’s next?","lvl2":"Summary"},"content":"Let Jupyter book tie this to the next (sequential) piece of content that people could move on to down below and in the sidebar. However, if this page uniquely enables your reader to tackle other nonsequential concepts throughout this book, or even external content, link to it here!\n\n","type":"content","url":"/notebooks/sklearn-clustering#whats-next","position":35},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/sklearn-clustering#resources-and-references","position":36},{"hierarchy":{"lvl1":"Project Pythia Notebook Template","lvl2":"Resources and references"},"content":"Finally, be rigorous in your citations and references as necessary. Give credit where credit is due. Also, feel free to link to relevant external material, further reading, documentation, etc. Then you’re done! Give yourself a quick review, a high five, and send us a pull request. A few final notes:\n\nKernel > Restart Kernel and Run All Cells... to confirm that your notebook will cleanly run from start to finish\n\nKernel > Restart Kernel and Clear All Outputs... before committing your notebook, our machines will do the heavy lifting\n\nTake credit! Provide author contact information if you’d like; if so, consider adding information here at the bottom of your notebook\n\nGive credit! Attribute appropriate authorship for referenced code, information, images, etc.\n\nOnly include what you’re legally allowed: no copyright infringement or plagiarism\n\nThank you for your contribution!","type":"content","url":"/notebooks/sklearn-clustering#resources-and-references","position":37},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn"},"type":"lvl1","url":"/notebooks/sklearn-regression","position":0},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn"},"content":"\n\n","type":"content","url":"/notebooks/sklearn-regression","position":1},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn"},"type":"lvl1","url":"/notebooks/sklearn-regression#regression-modeling-on-argo-using-scikit-learn","position":2},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn"},"content":"\n\n\n\n","type":"content","url":"/notebooks/sklearn-regression#regression-modeling-on-argo-using-scikit-learn","position":3},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/sklearn-regression#overview","position":4},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Overview"},"content":"Among machine learning approaches, RFR is a relatively simple algorithm that can be trained on regional datasets too small for deep learning. RFR has been successfully applied to a range of cases in oceanography (e.g. \\citet{sharp2022_GOBAIO2, callens2020_Using, tong2019_MultiFeature}), including for oxygen prediction in the Southern Ocean using BGC-Argo float data (\\citet{giglio2018_Estimating}). Furthermore, many oceanographic applications highlight RFR as a useful tool for geospatial observations because of its reduced overfitting tendency and ability to handle non-linear relationships between variables \\citep{zhou2023_Comparative, sharp2022_Monthly}. Deep learning methods are not necessarily better than simpler algorithms for data that are non-uniformly distributed; where multiple algorithms produce similar regional predictions, simple learners can offer greater stability and interpretability.\n\nThe objectives for this notebook are:\n\nTrain RFR on data from BGC-Argo ocean autonomous profilers and GO-SHIP bottle data\n\nValidate RFR to select the best model parameters, use cross-validation to assess overfitting\n\nTest RFR for an estimate of prediction error\n\nPredict missing variable on other profiling observations\n\nfrom IPython.display import Image\nImage(filename='../images/rfr_workflow.png') \n\n","type":"content","url":"/notebooks/sklearn-regression#overview","position":5},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/sklearn-regression#prerequisites","position":6},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Prerequisites"},"content":"This section was inspired by \n\nthis template of the wonderful \n\nThe Turing Way Jupyter Book.\n\nFollowing your overview, tell your reader what concepts, packages, or other background information they’ll need before learning your material. Tie this explicitly with links to other pages here in Foundations or to relevant external resources. Remove this body text, then populate the Markdown table, denoted in this cell with | vertical brackets, below, and fill out the information following. In this table, lay out prerequisite concepts by explicitly linking to other Foundations material or external resources, or describe generally helpful concepts.\n\nLabel the importance of each concept explicitly as helpful/necessary.\n\nConcepts\n\nImportance\n\nNotes\n\nIntro to Cartopy\n\nNecessary\n\n\n\nUnderstanding of NetCDF\n\nHelpful\n\nFamiliarity with metadata structure\n\nProject management\n\nHelpful\n\n\n\nTime to learn: estimate in minutes. For a rough idea, use 5 mins per subsection, 10 if longer; add these up for a total. Safer to round up and overestimate.\n\nSystem requirements:\n\nPopulate with any system, version, or non-Python software requirements if necessary\n\nOtherwise use the concepts table above and the Imports section below to describe required packages as necessary\n\nIf no extra requirements, remove the System requirements point altogether\n\n\n\n","type":"content","url":"/notebooks/sklearn-regression#prerequisites","position":7},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/sklearn-regression#imports","position":8},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Imports"},"content":"Begin your body of content with another --- divider before continuing into this section, then remove this body text and populate the following code cell with all necessary Python imports up-front:\n\n# Import packages\nimport numpy as np\nimport pandas as pd\nfrom scipy import interpolate\nimport scipy\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nfrom datetime import datetime, timedelta\nimport requests\nimport time\nimport os\nimport urllib3\nimport shutil\n# import tqdm\n\n# Import argopy functions\nimport argopy\nfrom argopy import DataFetcher  # This is the class to work with Argo data\nfrom argopy import ArgoIndex  #  This is the class to work with Argo index\nfrom argopy import ArgoNVSReferenceTables  # This is the class to retrieve data from Argo reference tables\nfrom argopy import ArgoColors  # This is a class with usefull pre-defined colors\nfrom argopy.plot import scatter_map, scatter_plot  # This is a function to easily make maps \nargopy.reset_options()\nargopy.clear_cache()\nargopy.set_options(cachedir='cache_bgc')\n# xr.set_options(display_expand_attrs = False)\n\n# Scikit-learn packages\nfrom scipy.stats import kde\nfrom scipy.stats import iqr\nfrom sklearn import linear_model\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support as score\n\n# Plotting Packages\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patch\nimport matplotlib.colors as mpcolors\nfrom matplotlib.colors import LogNorm\nfrom matplotlib.ticker import FormatStrFormatter\nimport matplotlib.patheffects as pe\nimport seaborn as sns\nimport gsw\nfrom cmocean import cm as cmo\n\n\n","type":"content","url":"/notebooks/sklearn-regression#imports","position":9},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Optional Background: Random Forest Regression"},"type":"lvl2","url":"/notebooks/sklearn-regression#optional-background-random-forest-regression","position":10},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Optional Background: Random Forest Regression"},"content":"Random Forest (RF) regression is a supervised machine learning method that averages the results over a “forest” of n decision trees for final prediction. Each decision tree determines a series of conditions at each node based on a set of observed variables, referred to as features. For regression, the trees find conditions that best separate the target observations into branches with the least variance.\n\nFormally, observations are sorted into j regions of predictor space (R_j) during training. Following the notation in James et al. (2013), each node in the tree considers a split into two new regions based on a condition s for a predictor X_j:\\begin{split}\nR_1(j,s) = \\{X|X_j < s\\} \\text{ and } R_2(j,s) = \\{X|X_j \\ge s\\}, \\\\\n\\end{split}\n\nand each region is assigned a prediction value that is the average of all target variable values (\\hat{y}_{R_j} ) within R_j. The condition s is chosen to minimize the residual sum of squares in the resulting bins:\\begin{split}\n\\sum_{i: \\: x_i \\in \\: R_1(j,s)}{} (y_i-\\hat{y}_{R_1})^2 \\; + \\sum_{i: \\: x_i \\in \\: R_2(j,s)}{} (y_i-\\hat{y}_{R_2})^2,\n\\end{split}\n\nwhere y_i represents each observed value. During testing, validation, and prediction, RF sorts input observations into the regions and assigns them the value \\hat{y}_{R_j}. One advantage of RF is that it limits the decision at each node split to a random subset of features. As a result, the trees in Random Forest are less correlated than those in a family of ``bagged’’ trees which consider all possible features at all nodes. The RF method is therefore less prone to overfitting, especially on spatiotemporally biased data (Stock \n\net.al (2022), Sharp et al. 2022).\n\n","type":"content","url":"/notebooks/sklearn-regression#optional-background-random-forest-regression","position":11},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Determine spatial and temporal bounds of Argo data"},"type":"lvl2","url":"/notebooks/sklearn-regression#determine-spatial-and-temporal-bounds-of-argo-data","position":12},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Determine spatial and temporal bounds of Argo data"},"content":"\n\nWe will use Argopy to set up some data to run RFR.\nAs an example, we access 3 years of North Atlantic data in 2019 through 2022.\nWe adapt this code from the example Argopy notebook are documented \n\nhere.\n\nOur approach downloads synthetic profiles, which are described in the previous notebook, \n\nAccessing Argo .\n\n# In the format [minlon, maxlon, minlat, maxlat, mindepth, maxdepth, start_date, end_date]\nBOX = [-56, -45, 54, 60, 0, 500, '2019-01', '2023-01']\nidx = ArgoIndex(index_file='bgc-s').load()  # Synthetic profiles\nidx\n\n\n# Select profile in a space/time domain:\nindex_BOX = [BOX[ii] for ii in [0, 1, 2, 3, 6, 7]]  # We don't want the pressure axis BOX limits\nidx.search_lat_lon_tim(index_BOX)\n\n\n# Get the list of all parameters for this region:\nprint('Parameters measured in the region: \\n' + str(idx.read_params())); print()\nprint ('There are ' + str(len(idx.read_wmo())) + ' floats in the region.')\n\n","type":"content","url":"/notebooks/sklearn-regression#determine-spatial-and-temporal-bounds-of-argo-data","position":13},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Fetching data using Argopy"},"type":"lvl2","url":"/notebooks/sklearn-regression#fetching-data-using-argopy","position":14},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Fetching data using Argopy"},"content":"\n\nThe following code fetches the actual data from the BOX bounds we used earlier. If using the parameters given here, the process takes about 6 minutes.\n\nf = DataFetcher(ds='bgc', mode='expert', params='all',\n                parallel=True, progress=True, cache=False,\n                chunks_maxsize={'time': 30},\n               )\nf = f.region(BOX).load()\nf\n\nAgain, the data is retrieved as an xarray Dataset. Metadata can be returned using the object index in a DataFrame.\n\n# Check the data structure (xarray.dataset):\nds = f.data # Dataset\ndf = f.index # Dataframe index\n\nWe can visualize where our training profiles are using a function included in Argopy.\nBy setting the parameter set_global=True, we can see the profiles in a global map context.\n\nscatter_map(df, traj=False, set_global=False, legend=False);\nax = plt.gca()\nax.set_title('Float Profile Locations (by WMO)');\n\n\n\na_param = 'PSAL'\nreftbl = ArgoNVSReferenceTables().tbl('R03')\nparam_info = reftbl[reftbl['altLabel']==a_param].iloc[0].to_dict()\nparam_info\n\n# To make the scatter map, we need to have the data mode available in one DataFrame column\n# so we need to add a new column with the DATA_MODE of the PARAMETER:\ndf[\"variables\"] = df[\"parameters\"].apply(lambda x: x.split())\ndf[\"%s_DM\" % a_param] = df.apply(lambda x: x['parameter_data_mode'][x['variables'].index(a_param)] if a_param in x['variables'] else '', axis=1)\n\n# Finally plot the map:\nfig, ax = scatter_map(df,\n                        hue=\"%s_DM\" % a_param,\n                        cmap=\"data_mode\",\n                        markersize=24,\n                        markeredgecolor='w',\n                        traj_color='gray',\n                        legend_title='Data mode')\nax.set_title(\"Data mode for %s (%s)\\n%i profiles from the %s\\n%i profiles downloaded\" % (param_info['prefLabel'], a_param, \n                                                                           idx.N_MATCH, idx.convention_title, df.shape[0]));\n\n","type":"content","url":"/notebooks/sklearn-regression#fetching-data-using-argopy","position":15},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl3":"Choose example parameter to visualize","lvl2":"Fetching data using Argopy"},"type":"lvl3","url":"/notebooks/sklearn-regression#choose-example-parameter-to-visualize","position":16},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl3":"Choose example parameter to visualize","lvl2":"Fetching data using Argopy"},"content":"\n\nWe can plot the adjusted Salinity over pressure, and color by QC flag.\n\nfig, ax = scatter_plot(ds, a_param + '_ADJUSTED_QC', this_x = a_param + '_ADJUSTED', \n                       vmin=0, vmax=9, cmap=ArgoColors('qc').cmap, figsize=(5,5)) #ArgoColors('qc').cmap\nax.set_title(\"QC for Adjusted %s [%s]\\n'%s' mission\" % (param_info['prefLabel'], a_param + '_ADJUSTED', f.mission), \n             fontdict={'weight': 'bold', 'size': 14});\n\n","type":"content","url":"/notebooks/sklearn-regression#choose-example-parameter-to-visualize","position":17},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Splitting data before training our model."},"type":"lvl2","url":"/notebooks/sklearn-regression#splitting-data-before-training-our-model","position":18},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Splitting data before training our model."},"content":"\n\nDuring RFR development, observations are separated into different datasets for training, validation, and testing steps (see Workflow schematic above). We note that the observations within a single glider or float profile are highly correlated; since the individual profiles are the ‘‘independent’’ units here rather than the pointwise samples, we keep observations from vertical profiles together during data splitting.\n\n# Prepare dataframe before splitting\nargodat = ds.argo.point2profile()\nargodat = argodat.to_dataframe().reset_index()\nargodat.columns\n\n# Run QC\nqcvars = ['DOXY_ADJUSTED_QC', 'PSAL_ADJUSTED_QC', 'TEMP_ADJUSTED_QC', 'PRES_ADJUSTED_QC', 'TIME_QC']\nprint(str(len(argodat)) + ' obs before QC')\nX = len(argodat)\n\nfor var in qcvars:\n    argodat = argodat[(argodat[var] == 1) | (argodat[var] == 2)| (argodat[var] == 8)]\n    #qc flag meanings           1: good data, 2: probably good data, 8: interpolated value\n    print('after ' + var)\n    print('\\t' + str(len(argodat)) + ' obs left   ' + ' \\t' +str(X-len(argodat)) + ' dropped'); \n    X = len(argodat)\n\nargodat.describe();\n\n\n#  Add training variables \n\ndef datetime2ytd(time, year = 2019):\n    \"\"\"\" Return time in YTD format from datetime format.\"\"\"\n    return (time - np.datetime64('2019-01-01'))/np.timedelta64(1, 'D')\n\nargodat['YD'] = datetime2ytd(argodat['TIME'])\nargodat['CT'] = gsw.CT_from_t(argodat['PSAL_ADJUSTED'], argodat['TEMP_ADJUSTED'], argodat['PRES_ADJUSTED'])\nargodat['SA']= gsw.SA_from_SP(argodat['PSAL_ADJUSTED'],argodat['PRES_ADJUSTED'],argodat['LONGITUDE'],argodat['LATITUDE'])\nargodat['SIGMA0'] = gsw.sigma0(argodat.SA.values, argodat.CT.values)\nargodat['SPICE'] = gsw.spiciness0(argodat.SA.values, argodat.CT.values)\n\ndvars = ['N_PROF','TIME', 'YD', 'LATITUDE', 'LONGITUDE', 'PRES_ADJUSTED','PSAL_ADJUSTED', 'TEMP_ADJUSTED', \n         'CT', 'SA', 'SIGMA0', 'SPICE', 'DOXY_ADJUSTED']\nargo_df = argodat[dvars]\n\nargo_df\n\n","type":"content","url":"/notebooks/sklearn-regression#splitting-data-before-training-our-model","position":19},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Begin preparing data for training"},"type":"lvl2","url":"/notebooks/sklearn-regression#begin-preparing-data-for-training","position":20},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Begin preparing data for training"},"content":"\n\ndef split_profiles(floatDF):\n    \"\"\" \n    @param floatDF: dataframe with all float data\n\n    @return: \n        training_data: training data (80% of profiles) with ship added\n        validation_data: validation data (20% of profiles)\n    \n    This method treats a float profile like the smallest discrete \"unit\" upon which to train and validate the model.\n    Our goal is to predict a water column (profile) rather than a discrete observation, so profiles are kept together while splitting.\n    Note that test data will be all profiles from the SOGOS float when test_split is set as False by default.\n\n    \"\"\"\n    # Create list of unique profile ID's and select random 80% for training\n    profnum = pd.unique(floatDF.N_PROF)\n\n    np.random.seed(42) \n    training_pnum = np.random.choice(profnum, int(np.floor(0.8*len(profnum))), replace=False)\n    training_data = floatDF[floatDF['N_PROF'].isin(training_pnum)]\n\n    # Take HALF of remaining profiles that were not in training set, for validation (10% of total)\n    pnum_vt = [x for x in profnum if x not in training_pnum]  # remaining profiles after training data removed\n\n    validation_pnum= np.random.choice(pnum_vt, int(np.floor(0.5*len(pnum_vt))), replace=False)\n    validation_data = floatDF[floatDF['N_PROF'].isin(validation_pnum)]\n\n    test_pnum = [x for x in pnum_vt if x not in validation_pnum]\n    test_data = floatDF[floatDF['N_PROF'].isin(test_pnum)]\n\n    return training_data, validation_data, test_data\n\n[training, validation, test] = split_profiles(argo_df)\n\nprint('# of total profiles: ' + str(len(argo_df.N_PROF.unique())))\nprint('# training observations: \\t' + str(len(training)))\nprint('# validation observations: \\t' + str(len(validation)))\nprint('# test observations: \\t\\t' + str(len(test)))\n\ntraining.head()\n\n","type":"content","url":"/notebooks/sklearn-regression#begin-preparing-data-for-training","position":21},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Set up model versions to try."},"type":"lvl2","url":"/notebooks/sklearn-regression#set-up-model-versions-to-try","position":22},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Set up model versions to try."},"content":"\n\nTo demonstrate, we try our model on three different simple versions. Real models should use more thorough methods to determine the optimal feature list.\n\nvar_list = {\n            'Model_A': ['SPICE', 'SIGMA0'],\n            'Model_B': ['CT', 'SA', 'PRES_ADJUSTED'], \n            'Model_C': [\"CT\", \"SA\", 'PRES_ADJUSTED', 'YD', 'LATITUDE', 'LONGITUDE'],\n            }\n\nmodel_list = list(var_list.keys())\n\n# Create dictionary of features\nallvars = []\nfor k in var_list.keys():\n    allvars = allvars + var_list[str(k)]\nallvars = list(set(allvars)) # remove duplicates\n\npal = sns.color_palette('Set2', n_colors = len(model_list))\nmodel_palettes = {k:v for k, v in zip(model_list, pal)}\n\n","type":"content","url":"/notebooks/sklearn-regression#set-up-model-versions-to-try","position":23},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl3":"Main RFR training method","lvl2":"Set up model versions to try."},"type":"lvl3","url":"/notebooks/sklearn-regression#main-rfr-training-method","position":24},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl3":"Main RFR training method","lvl2":"Set up model versions to try."},"content":"\n\nvar_predict = 'DOXY_ADJUSTED'\ndef train_RFR(var_list, training, validation, test, ntrees=1000, max_feats = 1/3, min_samples_split=5):\n    \"\"\" \n    Main method to train the RF model.\n    Update 04.18.24: Redo bootstrapping. \n    @param: \n        var_list: list of variables to use in the model\n        training: training data unscaled, i.e. original range of values\n        validation: validation data unscaled\n        test: test data unscaled \n        ntrees: 1000 trees by default.\n\n    @return:\n        Mdl: trained RF model\n        Mdl_MAE: Rescaled mean absolute error for training, validation, and test sets\n        Mdl_IQR: Rescaled IQR for training, validation, and test sets\n        DF_with_error: Dataframe with error metrics for the *TEST* set\n        validation_error: Dataframe with error metrics for the *VALIDATION* set\n    \"\"\"\n\n    Mdl = RandomForestRegressor(ntrees, max_features = max_feats, random_state = 0, bootstrap=True, min_samples_split=min_samples_split)\n        #  max_features: use at most X features at each split (m~sqrt(total features))\n\n    # Drop NaN's without profid or wmoid\n    cols_na = [col for col in training.columns if col not in ['profid', 'wmoid', 'AOU', 'dist_maxb']]\n    training_nona = training.dropna(axis=0, subset=cols_na)  # makes same length as training_scaled\n    validation_nona = validation.dropna(axis=0, subset=cols_na)\n    test_nona = test.dropna(axis=0, subset=cols_na)\n\n    # Create X Variables for each subset of data. Scale down. \n    X_training = training_nona[var_list].to_numpy()\n    X_validation = validation_nona[var_list].to_numpy()\n    X_test = test_nona[var_list].to_numpy()\n\n    # Nitrate, the target variable. \n    Y_training = training_nona[var_predict].to_numpy()\n    Y_validation = validation_nona[var_predict].to_numpy()\n    Y_test = test_nona[var_predict].to_numpy()\n\n    # Train the model\n    Mdl.fit(X_training, Y_training)\n\n    # Estimate\n    Y_pred_training = Mdl.predict(X_training)\n    Y_pred_validation = Mdl.predict(X_validation)\n    Y_pred_test = Mdl.predict(X_test)\n\n    # Create dataframe for the test set with depth --> \n    DF_with_error = test_nona.copy(); \n    DF_with_error = DF_with_error.reset_index(drop=True)\n    observed_nitrate = DF_with_error[var_predict].to_numpy()\n\n    # Save new dataframe with test results\n    DF_with_error['test_prediction'] = Y_pred_test\n    DF_with_error['test_error'] = DF_with_error['test_prediction'] - observed_nitrate\n    DF_with_error['test_relative_error'] = DF_with_error['test_error']/observed_nitrate\n\n    # Error metrics\n    AE_RF_training = np.abs(Y_pred_training - training_nona[var_predict])\n    IQR_RF_training = iqr(abs(AE_RF_training))\n    r2_RF_training = r2_score(training_nona[var_predict], Y_pred_training)\n\n    # Return validation metrics\n    AE_RF_validation = np.abs(Y_pred_validation - validation_nona[var_predict])\n    IQR_RF_validation = iqr(abs(AE_RF_validation))\n    r2_RF_validation = r2_score(validation_nona[var_predict], Y_pred_validation)\n\n    validation_error = validation_nona.copy() #pd.DataFrame()\n    validation_error['val_error'] = Y_pred_validation - validation_nona[var_predict]\n    validation_error['val_relative_error'] = validation_error['val_error']/validation_nona[var_predict]\n\n    AE_RF_test = np.abs(Y_pred_test - test_nona[var_predict]) # same as DF_with_error['test_error']\n    IQR_RF_test = iqr(abs(AE_RF_test))\n    r2_RF_test = r2_score(test_nona[var_predict], Y_pred_test)\n\n    Mdl_MAE = [np.nanmedian(abs(AE_RF_training)), np.nanmedian(abs(AE_RF_validation)), np.nanmedian(abs(AE_RF_test))]\n    Mdl_IQR = [IQR_RF_training, IQR_RF_validation, IQR_RF_test]\n    Mdl_r2 = [r2_RF_training, r2_RF_validation, r2_RF_test]\n\n    return [Mdl, Mdl_MAE, Mdl_IQR, Mdl_r2, DF_with_error, validation_error]\n\nNext, we train the model using our main train_RFR() function.\nDepending on your dataset size and model lists, this may take several minutes (here, about 8 minutes).\n\nNote that use a relatively small number of decision tres (n=500) to speed up our example.\n\n# Train the models with chosen lists.\n# Set parameters\nmodels = dict.fromkeys(model_list)\nmodels_MAE = dict.fromkeys(model_list)\nmodels_IQR = dict.fromkeys(model_list)\nmodels_r2 = dict.fromkeys(model_list)\nmodels_DF_err = dict.fromkeys(model_list)\nmodels_val_err = dict.fromkeys(model_list)\n\n\nntrees=500\nmax_feats = 1/3\nmin_samples_split = 5\ndescription = 'ntrees=' + str(ntrees) + ', max_features=' + str(max_feats)\n\nprint(description + '\\n')\nfor k in model_list: \n    [models[k], models_MAE[k], models_IQR[k], models_r2[k], models_DF_err[k], models_val_err[k]] = train_RFR(var_list[k], training, validation, test, \n                                                                                         ntrees = ntrees, max_feats = max_feats,\n                                                                                         min_samples_split=min_samples_split)\n\nRFR_results = [models, models_MAE, models_IQR, models_r2, models_DF_err, models_val_err]\n\n","type":"content","url":"/notebooks/sklearn-regression#main-rfr-training-method","position":25},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Feature Importance"},"type":"lvl2","url":"/notebooks/sklearn-regression#feature-importance","position":26},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Feature Importance"},"content":"\n\nRFR returns a measure for each feature called ``feature importance’’ that reflects its predictive value. For each split where a given feature is used to determine the split criterion, the difference in the variance of the pre-split node compared to the two post-split nodes is calculated (and weighted by the number of samples in the pre-split node). These values are summed across all of the relevant splits in a given decision tree; the average of these sums across all trees represents the feature importance of the given variable. The feature importances are computed for all variables used in the model, and are scaled relative to each other so that the sum of all of the final feature importances is 1. The feature importances are often screened during RFR development to help select a final set of features.\n\nallvars = ['SPICE', 'SIGMA0', 'CT', 'SA', 'PRES_ADJUSTED', 'YD', 'LATITUDE', 'LONGITUDE']\n\n# Make total feature importance dictionary for plotting\ndict = dict.fromkeys(allvars)\nfeat_imps = pd.DataFrame()\n\n# Fill in feature importance\nfor ind, vkey in enumerate(model_list):\n    for var in var_list[vkey]:\n        feat_imps.at[ind, var] = models[vkey].feature_importances_[var_list[vkey].index(var)]\n\n# feat_imps = feat_imps.set_index('Mdl').fillna(np.nan)\nfeat_imps['Mdl'] = model_list\nfeat_imps = feat_imps.set_index('Mdl').fillna(np.nan)\nfeat_imps\n\nWe can compare feature importances across models.\n\npal = sns.color_palette('Set2', n_colors = len(model_list))\nmodel_palettes = {k:v for k, v in zip(model_list, pal)}\n\nfig = plt.figure(figsize=(5,3), layout='constrained')\nax = fig.gca()\n\nbarwidth=.8\n\nmodlist = model_list\nvarslist = allvars\nxlabel = ['spice', 'sigma', 'temp', 'salinity', 'pres', 'time', 'lat', 'lon']\n\n# Result of plot method\nxind = dict.fromkeys(modlist)\nxind[modlist[0]] = np.arange(len(varslist))*len(modlist)\n\nfor i in range(len(modlist)-1):\n    xind[modlist[i+1]] = [x + barwidth for x in xind[modlist[i]]]\n\nfor modtag in modlist:\n    ax.bar(xind[modtag], feat_imps[varslist].loc[modtag].to_numpy(), color = model_palettes[modtag], alpha=0.8, label = modtag, zorder=3)\n\nax.grid(axis='y', zorder=0)\nax.legend(loc='upper right')\n\nax.set_title('Feature Importances by Model')\nax.set_xticks(xind[modlist[2]])\nax.set_xticklabels(varslist)\nax.set_xticklabels(xlabel);\n\n","type":"content","url":"/notebooks/sklearn-regression#feature-importance","position":27},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Validation Error Analysis"},"type":"lvl2","url":"/notebooks/sklearn-regression#validation-error-analysis","position":28},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Validation Error Analysis"},"content":"\n\ndef get_model_metrics(resultlist, mod_list=model_list):\n    \"\"\" \n    resultlist: typically list of [models, models_MAE, models_IQR, models_r2, models_DF_err]\"\"\"\n    models_MAE=resultlist[1]\n    models_IQR=resultlist[2]\n    models_r2=resultlist[3]\n\n    model_metrics = pd.DataFrame()\n    model_metrics['validation_MAE'] = [x[1][1].item() for x in models_MAE.items()]\n    model_metrics['validation_IQR'] = [x[1][1].item() for x in models_IQR.items()]\n\n    model_metrics['test_MAE'] = [x[1][2].item() for x in models_MAE.items()]\n    model_metrics['test_IQR'] = [x[1][2].item() for x in models_IQR.items()]\n\n    model_metrics['model'] = mod_list\n    model_metrics.set_index('model', inplace=True)\n    \n    return model_metrics\n\n# Method to print MAE dictionaries\ndef print_dict(dict):\n    print(\"\\n\".join(\"{}  \\t{}\".format(k, v) for k, v in dict.items()))\n\ndef print_sorted(dict):\n    print(\"\\n\".join(\"{}  \\t{}\".format(k, v) for k, v in sorted(dict.items(), key=lambda x:x[1]))) \n\nmodel_metrics = get_model_metrics(RFR_results, mod_list = RFR_results[1].keys()).sort_values(by='validation_MAE')\nmodel_metrics.sort_values(by='validation_MAE', ascending=False)[['validation_MAE', 'validation_IQR']]\n\n# Error printing methods\ndef get_95_bounds(data):\n    mean = np.mean(data); std_dev = np.std(data)\n    low = mean - 2 * std_dev\n    high = mean + 2 * std_dev\n\n    return [low, high]\n\n# Method to print errors restricted to depth \ndef print_error_depths(data, var='test_error', pres_lim = [0,500]):\n   data = data[(data.PRES_ADJUSTED > pres_lim[0]) & (data.PRES_ADJUSTED < pres_lim[1])]\n   err = data[var]\n\n   print('Errors between depths ' + str(pres_lim[0]) + ' to ' + str(pres_lim[1]) + ':')\n   print('median abs error: \\t' + str(np.abs(err).median()))\n   print('mean abs error \\t\\t' + str(np.abs(err).mean()))\n\n   # Bounds 95\n   [low, high] = get_95_bounds(err)\n   print('\\n95% of errors fall between:')\n   print(str(low.round(5)) + ' to ' + str(high.round(5)) )\n\nprint('Model_C') # leaf size: 5\nprint_error_depths(RFR_results[5]['Model_C'], var='val_relative_error', pres_lim = [0,500])\n\nWe can visualize the distribution of errors between models.\n\n\ntag = 'Model Validation Errors'\nvar = 'val_relative_error'\ndat = RFR_results[5]\nymax=60\n\n\nfig = plt.figure(figsize=(7,3))\nax = fig.gca()\n\nfor mod in model_list[-4:]:\n    RF = dat[mod][var]\n    ax.hist(RF, bins=40, alpha=0.5, color= model_palettes[mod], label=mod, density=True)\n\nax.vlines(0, ymin=0, ymax=ymax, colors='k', alpha=0.4, linestyles='dashed')\nax.set_ylim([0,ymax])\n\nplt.legend()\nax.set_xlabel('Relative Error %')\n\nax.grid(alpha=0.5, zorder=0)\nax.set_xlim([-.1, .1])\n\nax.set_title('Validation Error Distributions by Model')\n\n\n","type":"content","url":"/notebooks/sklearn-regression#validation-error-analysis","position":29},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Testing on withheld data"},"type":"lvl2","url":"/notebooks/sklearn-regression#testing-on-withheld-data","position":30},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Testing on withheld data"},"content":"\n\n\n\nThis is where you begin your first section of material, loosely tied to your objectives stated up front. Tie together your notebook as a narrative, with interspersed Markdown text, images, and more as necessary,\n\n# some subsection code\nnew = \"helpful information\"\n\n","type":"content","url":"/notebooks/sklearn-regression#testing-on-withheld-data","position":31},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl3":"Another content subsection","lvl2":"Testing on withheld data"},"type":"lvl3","url":"/notebooks/sklearn-regression#another-content-subsection","position":32},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl3":"Another content subsection","lvl2":"Testing on withheld data"},"content":"Keep up the good work! A note, try to avoid using code comments as narrative, and instead let them only exist as brief clarifications where necessary.\n\n","type":"content","url":"/notebooks/sklearn-regression#another-content-subsection","position":33},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Your second content section"},"type":"lvl2","url":"/notebooks/sklearn-regression#your-second-content-section","position":34},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Your second content section"},"content":"Here we can move on to our second objective, and we can demonstrate\n\n","type":"content","url":"/notebooks/sklearn-regression#your-second-content-section","position":35},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl3":"Subsection to the second section","lvl2":"Your second content section"},"type":"lvl3","url":"/notebooks/sklearn-regression#subsection-to-the-second-section","position":36},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl3":"Subsection to the second section","lvl2":"Your second content section"},"content":"","type":"content","url":"/notebooks/sklearn-regression#subsection-to-the-second-section","position":37},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl4":"a quick demonstration","lvl3":"Subsection to the second section","lvl2":"Your second content section"},"type":"lvl4","url":"/notebooks/sklearn-regression#a-quick-demonstration","position":38},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl4":"a quick demonstration","lvl3":"Subsection to the second section","lvl2":"Your second content section"},"content":"","type":"content","url":"/notebooks/sklearn-regression#a-quick-demonstration","position":39},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl5":"of further and further","lvl4":"a quick demonstration","lvl3":"Subsection to the second section","lvl2":"Your second content section"},"type":"lvl5","url":"/notebooks/sklearn-regression#of-further-and-further","position":40},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl5":"of further and further","lvl4":"a quick demonstration","lvl3":"Subsection to the second section","lvl2":"Your second content section"},"content":"","type":"content","url":"/notebooks/sklearn-regression#of-further-and-further","position":41},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl6":"header levels","lvl5":"of further and further","lvl4":"a quick demonstration","lvl3":"Subsection to the second section","lvl2":"Your second content section"},"type":"lvl6","url":"/notebooks/sklearn-regression#header-levels","position":42},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl6":"header levels","lvl5":"of further and further","lvl4":"a quick demonstration","lvl3":"Subsection to the second section","lvl2":"Your second content section"},"content":"\n\nas well m = a * t / h text! Similarly, you have access to other \\LaTeX equation \n\nfunctionality via MathJax (demo below from link),\\begin{align}\n\\dot{x} & = \\sigma(y-x) \\\\\n\\dot{y} & = \\rho x - y - xz \\\\\n\\dot{z} & = -\\beta z + xy\n\\end{align}\n\nCheck out \n\nany number of helpful Markdown resources for further customizing your notebooks and the \n\nJupyter docs for Jupyter-specific formatting information. Don’t hesitate to ask questions if you have problems getting it to look just right.\n\n","type":"content","url":"/notebooks/sklearn-regression#header-levels","position":43},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Last Section"},"type":"lvl2","url":"/notebooks/sklearn-regression#last-section","position":44},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Last Section"},"content":"If you’re comfortable, and as we briefly used for our embedded logo up top, you can embed raw html into Jupyter Markdown cells (edit to see):\n\nInfoYour relevant information here!\n\nFeel free to copy this around and edit or play around with yourself. Some other admonitions you can put in:\n\nSuccessWe got this done after all!\n\nWarningBe careful!\n\nDangerScary stuff be here.\n\nWe also suggest checking out Jupyter Book’s \n\nbrief demonstration on adding cell tags to your cells in Jupyter Notebook, Lab, or manually. Using these cell tags can allow you to \n\ncustomize how your code content is displayed and even \n\ndemonstrate errors without altogether crashing our loyal army of machines!\n\n\n\n","type":"content","url":"/notebooks/sklearn-regression#last-section","position":45},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/sklearn-regression#summary","position":46},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Summary"},"content":"Add one final --- marking the end of your body of content, and then conclude with a brief single paragraph summarizing at a high level the key pieces that were learned and how they tied to your objectives. Look to reiterate what the most important takeaways were.","type":"content","url":"/notebooks/sklearn-regression#summary","position":47},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/sklearn-regression#whats-next","position":48},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl3":"What’s next?","lvl2":"Summary"},"content":"Let Jupyter book tie this to the next (sequential) piece of content that people could move on to down below and in the sidebar. However, if this page uniquely enables your reader to tackle other nonsequential concepts throughout this book, or even external content, link to it here!\n\n","type":"content","url":"/notebooks/sklearn-regression#whats-next","position":49},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/sklearn-regression#resources-and-references","position":50},{"hierarchy":{"lvl1":"Regression Modeling on Argo using Scikit-learn","lvl2":"Resources and references"},"content":"Finally, be rigorous in your citations and references as necessary. Give credit where credit is due. Also, feel free to link to relevant external material, further reading, documentation, etc. Then you’re done! Give yourself a quick review, a high five, and send us a pull request. A few final notes:\n\nKernel > Restart Kernel and Run All Cells... to confirm that your notebook will cleanly run from start to finish\n\nKernel > Restart Kernel and Clear All Outputs... before committing your notebook, our machines will do the heavy lifting\n\nTake credit! Provide author contact information if you’d like; if so, consider adding information here at the bottom of your notebook\n\nGive credit! Attribute appropriate authorship for referenced code, information, images, etc.\n\nOnly include what you’re legally allowed: no copyright infringement or plagiarism\n\nThank you for your contribution!","type":"content","url":"/notebooks/sklearn-regression#resources-and-references","position":51}]}